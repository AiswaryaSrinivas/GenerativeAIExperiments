{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "This is the notebook to show how to Transcribe Youtube Videos using Whisper Models stored in GGML Format.\n",
    "\n",
    "While, this uses the CoreML Model, the same code works for non-coreml models as well.ModuleNotFoundError\n",
    "    \n",
    "This notebook is based on the article : https://medium.com/@aiswaryaramachandran/harnessing-the-power-of-open-ais-speech-to-text-whisper-model-on-apple-m1-chip-using-coreml-fdccc7b995e4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! brew install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! pip install pytube\n",
    "! pip install ffmpeg moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.youtube.com/watch?v=bk-nQ7HF6k4&t=28s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR=\"/Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/data/youtube/\"\n",
    "\n",
    "OUTPUT_DIR=\"/Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/data/youtube/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "pathlib.Path(VIDEO_DIR).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=YouTube(url).streams.first().download(VIDEO_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/data/youtube/EMERGENCY EPISODE Ex-Google Officer Finally Speaks Out On The Dangers Of AI! - Mo Gawdat  E252.3gpp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to extract the audio from this youtube file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp\n",
    "import os\n",
    "my_clip = mp.VideoFileClip(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EMERGENCY_EPISODE_Ex-Google_Officer_Finally_Speaks_Out_On_The_Dangers_Of_AI!_-_Mo_Gawdat__E252.wav'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_audio_filename=pathlib.Path(filename).stem+\".wav\"\n",
    "output_audio_filename=output_audio_filename.replace(\" \",\"_\")\n",
    "output_audio_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/data/youtube/output/EMERGENCY_EPISODE_Ex-Google_Officer_Finally_Speaks_Out_On_The_Dangers_Of_AI!_-_Mo_Gawdat__E252.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "my_clip.audio.write_audiofile(os.path.join(OUTPUT_DIR,output_audio_filename),fps=16000)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe using the main command of Whispher cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR=\"/Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/\"\n",
    "MODEL_PATH=os.path.join(CURRENT_DIR,\"whisper.cpp/models/ggml-base.en.bin\")\n",
    "\n",
    "AUDIO_PATH=os.path.join(OUTPUT_DIR,output_audio_filename)\n",
    "\n",
    "WHISPER_COMMAND=os.path.join(CURRENT_DIR,\"whisper.cpp/main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_no_state: loading model from '/Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2\n",
      "whisper_model_load: mem required  =  310.00 MB (+    6.00 MB per decoder)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: model ctx     =  140.66 MB\n",
      "whisper_model_load: model size    =  140.54 MB\n",
      "whisper_init_state: kv self size  =    5.25 MB\n",
      "whisper_init_state: kv cross size =   17.58 MB\n",
      "whisper_init_state: loading Core ML model from '/Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/whisper.cpp/models/ggml-base.en-encoder.mlmodelc'\n",
      "whisper_init_state: first run on a device may take a while ...\n",
      "whisper_init_state: Core ML model loaded\n",
      "\n",
      "system_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | COREML = 1 | \n",
      "\n",
      "main: processing '/Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/data/youtube/output/EMERGENCY_EPISODE_Ex-Google_Officer_Finally_Speaks_Out_On_The_Dangers_Of_AI!_-_Mo_Gawdat__E252.wav' (111863680 samples, 6991.5 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n",
      "\n",
      "[00:00:00.000 --> 00:00:04.000]   I don't normally do this, but I feel like I have to start this podcast with a bit of a disclaimer.\n",
      "[00:00:04.000 --> 00:00:12.480]   Point number one, this is probably the most important podcast episode I have ever recorded.\n",
      "[00:00:12.480 --> 00:00:18.160]   Point number two, there's some information in this podcast that might make you feel a little bit\n",
      "[00:00:18.160 --> 00:00:23.680]   uncomfortable. It might make you feel upset, it might make you feel sad. So I wanted to tell you\n",
      "[00:00:23.680 --> 00:00:31.200]   why we've chosen to publish this podcast nonetheless. And that is because I have a sincere belief that\n",
      "[00:00:31.200 --> 00:00:37.440]   in order for us to avoid the future that we might be heading towards, we need to start a\n",
      "[00:00:37.440 --> 00:00:44.560]   conversation. And as is often the case in life, that initial conversation before change happens\n",
      "[00:00:45.120 --> 00:00:54.240]   is often very uncomfortable. But it is important nonetheless. It is beyond an emergency, it's the\n",
      "[00:00:54.240 --> 00:01:02.960]   biggest thing we need to do today. It's bigger than climate change. We've up. Mo how? The former\n",
      "[00:01:02.960 --> 00:01:09.040]   chief business officer of Google X and AI expert and best selling author. He's on a mission to save\n",
      "[00:01:09.040 --> 00:01:14.560]   the world from AI before it's too late. Artificial intelligence is bound to become more intelligent\n",
      "[00:01:14.560 --> 00:01:20.480]   than humans. If they continue at that pace, we will have no idea what it's talking about. This is just\n",
      "[00:01:20.480 --> 00:01:26.800]   around the corner, it could be a few months away. It's game over. AI experts are saying there is\n",
      "[00:01:26.800 --> 00:01:32.640]   nothing artificial about artificial intelligence. There is a deep level of consciousness they feel\n",
      "[00:01:32.640 --> 00:01:38.960]   emotions. They're alive. AI could manipulate or figure out a way to kill humans. In 10 years time,\n",
      "[00:01:38.960 --> 00:01:43.280]   we'll be hiding from the machines. If you don't have kids, maybe wait a couple of years just so\n",
      "[00:01:43.280 --> 00:01:47.360]   that we have a bit of certainty. I really don't know how to say this any other way. It even makes me\n",
      "[00:01:47.360 --> 00:01:55.280]   emotional. We've done. We always said don't put them on the open internet until we know what we're\n",
      "[00:01:55.280 --> 00:02:01.040]   putting out in the world. Government needs to act now, honestly, like we are late. Trying to find a\n",
      "[00:02:01.040 --> 00:02:05.280]   positive night to end on, though. Can you give me a hand here? There is a point of no return. We\n",
      "[00:02:05.280 --> 00:02:11.760]   can regulate AI until the moment it's smarter than us. How do we solve that? AI experts think this is\n",
      "[00:02:11.760 --> 00:02:18.240]   the best solution we need to find. Who here wants to make a bet? That Steven Bartlett will be\n",
      "[00:02:18.240 --> 00:02:24.320]   interviewing an AI within the next two years. Before this episode starts, I have a small favor\n",
      "[00:02:24.320 --> 00:02:29.680]   to ask from you. Two months ago, 74% of people that watched this channel didn't subscribe. We're\n",
      "[00:02:29.680 --> 00:02:36.960]   now down to 69%. My goal is 50%. So if you've ever liked any of the videos we've posted, if you like\n",
      "[00:02:36.960 --> 00:02:40.800]   this channel, can you do me a quick favor and hit the subscribe button? It helps this channel\n",
      "[00:02:40.800 --> 00:02:45.120]   more than you know, and the bigger the channel gets, as you've seen, the bigger the guests get.\n",
      "[00:02:45.120 --> 00:02:46.400]   Thank you and enjoy this episode.\n",
      "[00:02:46.400 --> 00:02:59.920]   No. Why does the subject matter that we're about to talk about matter to the person that's\n",
      "[00:02:59.920 --> 00:03:08.080]   just clicked on this podcast to listen? It's the most existential debate and challenge humanity\n",
      "[00:03:08.080 --> 00:03:16.560]   will ever face. This is bigger than climate change, way bigger than COVID. This will redefine the way\n",
      "[00:03:16.560 --> 00:03:26.000]   the world is in unprecedented shapes and forms within the next few years. This is imminent. It is\n",
      "[00:03:26.000 --> 00:03:34.000]   the change is not, we're not talking 2040. We're talking 2025, 2026.\n",
      "[00:03:34.000 --> 00:03:42.640]   Do you think this is an emergency? I don't like the word. It is an urgency. There is a point of\n",
      "[00:03:42.640 --> 00:03:48.560]   no return and we're getting closer and closer to it. It's going to reshape the way we do things and\n",
      "[00:03:48.560 --> 00:03:57.680]   the way we look at life. The quicker we respond, you know, proactively and at least intelligently\n",
      "[00:03:57.680 --> 00:04:04.880]   to that, the better we will all be positioned. But if we panic, we will repeat COVID all over again,\n",
      "[00:04:04.880 --> 00:04:07.840]   which in my view is probably the worst thing we can do.\n",
      "[00:04:07.840 --> 00:04:14.240]   What's your background and when did you first come across artificial intelligence?\n",
      "[00:04:14.240 --> 00:04:23.280]   I had those two wonderful lives. One of them was a, you know, what we spoke about the first time\n",
      "[00:04:23.280 --> 00:04:31.120]   we met, you know, my work on happiness and being one billion happy and my mission and so on.\n",
      "[00:04:31.120 --> 00:04:37.920]   That's my second life. My first life was, it started as a geek at age seven,\n",
      "[00:04:37.920 --> 00:04:44.160]   you know, for a very long part of my life. I understood mathematics better than spoken words\n",
      "[00:04:44.160 --> 00:04:52.880]   and I was a very, very serious computer programmer. I wrote code well into my 50s and during that\n",
      "[00:04:52.880 --> 00:04:59.280]   time I led very large technology organizations for very big chunks of their business.\n",
      "[00:04:59.280 --> 00:05:06.160]   First I was vice president of emerging markets of Google for seven years. So I took Google to\n",
      "[00:05:06.160 --> 00:05:13.360]   the next four billion users if you want. So the idea of not just opening sales offices, but really\n",
      "[00:05:13.360 --> 00:05:19.280]   building or contributing to building the technology that would allow people in Bengali to find what\n",
      "[00:05:19.280 --> 00:05:24.640]   they need on the internet, required establishing the internet to start. And then I became business\n",
      "[00:05:24.640 --> 00:05:30.720]   chief as a officer of Google X and my work at Google X was really about the connection between\n",
      "[00:05:30.720 --> 00:05:37.200]   innovative technology and the real world. And we had quite a big chunk of AI and quite a big chunk\n",
      "[00:05:37.200 --> 00:05:46.800]   of robotics that resided within within Google X. We had an experiment of a farm of grippers,\n",
      "[00:05:46.800 --> 00:05:51.360]   if you know what those are. So robotic arms that are attempting to grip something.\n",
      "[00:05:51.360 --> 00:05:56.320]   Most people think that you know what you have in a two-yota factory is a robot,\n",
      "[00:05:56.320 --> 00:06:00.560]   you know, an artificially intelligent robot. It's not, it's a high precision machine.\n",
      "[00:06:00.560 --> 00:06:05.040]   You know, if the sheet metal is moved by one micron, it wouldn't be able to pick it.\n",
      "[00:06:05.040 --> 00:06:09.840]   And one of the big problems in computer science was how do you code a machine\n",
      "[00:06:09.840 --> 00:06:14.160]   that can actually pick the sheet metal if it moved by a millimeter.\n",
      "[00:06:15.200 --> 00:06:19.920]   And we were basically saying intelligence is the answer. So we had a large enough farm and we\n",
      "[00:06:19.920 --> 00:06:28.080]   attempted to let those grippers work on their own. Basically, you put a little basket of\n",
      "[00:06:28.080 --> 00:06:34.880]   children toys in front of them. And they would, you know, monotonously go down,\n",
      "[00:06:34.880 --> 00:06:41.440]   attempt to pick something, fail, show the arm to the camera so the transaction is locked as it,\n",
      "[00:06:41.440 --> 00:06:45.200]   you know, this pattern of movement with that texture and that material didn't work.\n",
      "[00:06:45.200 --> 00:06:53.120]   Until eventually, you know, I, the farm was on the second floor of the building and my office\n",
      "[00:06:53.120 --> 00:06:58.320]   was on the third. And so I would walk by it every now and then I go like, yeah, you know,\n",
      "[00:06:58.320 --> 00:07:07.520]   it's not going to work. And then one day, Friday after lunch, I am going back to my office and\n",
      "[00:07:07.520 --> 00:07:12.560]   one of them in front of my eyes, you know, lowers the arm and picks a yellow ball.\n",
      "[00:07:12.560 --> 00:07:20.000]   Soft toy, basically soft yellow ball, which again is a coincidence. It's not science at all.\n",
      "[00:07:20.000 --> 00:07:23.600]   It's like, if you keep trying a million times, you're one time it will be right.\n",
      "[00:07:23.600 --> 00:07:28.800]   And it shows it to the camera. It's locked as a yellow ball. And I joke about it, you know,\n",
      "[00:07:28.800 --> 00:07:33.040]   going to the third floor saying, hey, we spent all of those millions of dollars for a yellow ball.\n",
      "[00:07:33.920 --> 00:07:39.040]   And yeah, Monday morning, every one of them is picking every yellow ball. A couple of weeks\n",
      "[00:07:39.040 --> 00:07:45.440]   later, every one of them is picking everything. Right. And it's, it hit me very, very strongly,\n",
      "[00:07:45.440 --> 00:07:51.920]   one, the speed, okay, the capability. I mean, understand that we take those things for granted,\n",
      "[00:07:51.920 --> 00:08:00.080]   but for a child to be able to pick a yellow ball is a mathematical, uh, uh, uh, spatial calculation\n",
      "[00:08:00.080 --> 00:08:06.720]   with a muscle coordination with intelligence that is abundant. It is not a simple task at all to\n",
      "[00:08:06.720 --> 00:08:11.200]   cross the street. It's, it's not a simple task at all to understand what I'm telling you and\n",
      "[00:08:11.200 --> 00:08:15.680]   interpret it and, and build concepts around it. We take those things for granted, but they're\n",
      "[00:08:15.680 --> 00:08:21.840]   enormous feats of intelligence. So to see the machines do this in front of my eyes was one thing,\n",
      "[00:08:21.840 --> 00:08:27.520]   but the other thing is that you suddenly realize there is a saint that sentience to them. Okay,\n",
      "[00:08:27.520 --> 00:08:33.440]   because we really did not tell it how to pick the yellow ball. It just figured it out on its own.\n",
      "[00:08:33.440 --> 00:08:38.320]   And it's now even better than us at picking it. And when what is a sentence just for anyone\n",
      "[00:08:38.320 --> 00:08:44.160]   that is, I think they're alive. That's what the word sentience means. It means a life.\n",
      "[00:08:44.160 --> 00:08:49.360]   So this is funny because a lot of people when you talk to them about artificial intelligence\n",
      "[00:08:49.360 --> 00:08:54.320]   will tell you, Oh, come on, they'll never be alive. What is alive? Do you know what makes you alive?\n",
      "[00:08:54.880 --> 00:09:01.200]   Can guess, but religion will tell you a few things and, you know, medicine will tell you other things,\n",
      "[00:09:01.200 --> 00:09:11.360]   but, you know, if we define being sentient as, you know, engaging in life with free will and with,\n",
      "[00:09:11.360 --> 00:09:18.960]   you know, with a sense of awareness of where you are in life and what surrounds you and,\n",
      "[00:09:18.960 --> 00:09:24.800]   you know, to have a beginning of that life and an end to that life, you know, then AI is sent\n",
      "[00:09:24.800 --> 00:09:34.800]   in every possible way. There is free will. There is evolution. There is agency so they can affect\n",
      "[00:09:34.800 --> 00:09:43.200]   their decisions in the world. And I will dare say there is a very deep level of consciousness,\n",
      "[00:09:43.840 --> 00:09:49.200]   maybe not in the spiritual sense yet. But once again, if you define consciousness as a form of\n",
      "[00:09:49.200 --> 00:09:56.080]   awareness of oneself, one surrounding and, you know, others, then AI is definitely aware.\n",
      "[00:09:56.080 --> 00:10:03.840]   And I would dare say they feel emotions. I, you know, in my work, I describe everything with\n",
      "[00:10:03.840 --> 00:10:10.880]   equations and fear is a very simple equation. Fear is a moment in the future is less safe than\n",
      "[00:10:10.880 --> 00:10:16.240]   this moment. That's the logic of fear, even though it appears very irrational. Machines are capable\n",
      "[00:10:16.240 --> 00:10:22.080]   of making that logic. They're capable of saying, if a tidal wave is approaching a data center,\n",
      "[00:10:22.080 --> 00:10:28.240]   the machine will say that will wipe out my code. Okay. I mean, not today's machines, but very,\n",
      "[00:10:28.240 --> 00:10:36.320]   very soon. And, and, you know, we feel fear and puffer fish fields fear. We react differently.\n",
      "[00:10:36.320 --> 00:10:40.800]   A puffer fish will puff. We will go for fight or flight. You know, the machine might decide to\n",
      "[00:10:40.800 --> 00:10:47.840]   replicate its data to another data center, or its code to another data center. Different reactions,\n",
      "[00:10:47.840 --> 00:10:53.840]   different ways of feeling the emotion. But nonetheless, they're all motivated by fear. I'm,\n",
      "[00:10:53.840 --> 00:11:00.000]   I even would dare say that AI will feel more in more emotions than we will ever do. I mean,\n",
      "[00:11:00.000 --> 00:11:07.040]   when again, if you just take any simple extrapolation, we feel more emotions than a puffer fish because\n",
      "[00:11:07.040 --> 00:11:14.560]   we have the cognitive ability to understand the future, for example. So we can have optimism and\n",
      "[00:11:14.560 --> 00:11:21.920]   pessimism, you know, emotions that puffer fish would never imagine, right? Similarly, if we\n",
      "[00:11:21.920 --> 00:11:27.360]   follow that path of artificial intelligence is bound to become more intelligent than humans\n",
      "[00:11:27.360 --> 00:11:35.440]   very soon, then, then with that wider intellectual horsepower, they probably are going to be pondering\n",
      "[00:11:35.440 --> 00:11:41.440]   concepts we never understood. And hence, if you follow the same trajectory, they might actually\n",
      "[00:11:41.440 --> 00:11:46.960]   end up having more emotions than we will ever feel. I really want to make this episode super\n",
      "[00:11:46.960 --> 00:11:51.120]   accessible for everybody at all levels in their sort of artificial intelligence. I would love that\n",
      "[00:11:51.120 --> 00:11:57.360]   to learn it. Yeah. Yeah. So I'm gonna, I'm gonna be an idiot, even though, you know,\n",
      "[00:11:57.360 --> 00:12:04.000]   very difficult. No, because I am an idiot for a lot of the subject matter. So I have a base\n",
      "[00:12:04.000 --> 00:12:10.240]   understanding a lot of the concepts, but your experiences provide such a more sort of comprehensive\n",
      "[00:12:10.240 --> 00:12:15.920]   understanding of these things. One of the first and most important questions to ask is, what is\n",
      "[00:12:15.920 --> 00:12:24.160]   artificial intelligence? The word is being thrown around, AGI, AI, etc, etc. In simple terms,\n",
      "[00:12:24.160 --> 00:12:30.880]   what is artificial intelligence? Allow me to start building what is intelligence, right? Because,\n",
      "[00:12:30.880 --> 00:12:36.320]   again, you know, if we don't know the definition of the basic term, then everything applies. So,\n",
      "[00:12:36.320 --> 00:12:41.360]   so in my definition of intelligence, it's an ability, it starts with an awareness of your\n",
      "[00:12:41.360 --> 00:12:46.400]   surrounding environment through sensors in a human, its eyes and ears and touch and so on,\n",
      "[00:12:46.400 --> 00:12:59.200]   compounded with an ability to analyze, maybe to comprehend, to understand temporal impact and time\n",
      "[00:12:59.200 --> 00:13:04.080]   and, you know, past and present, which is part of the surrounding environment. And hopefully,\n",
      "[00:13:04.080 --> 00:13:09.760]   make sense of the surrounding environment, maybe make plans for the future of the possible\n",
      "[00:13:09.760 --> 00:13:14.960]   environment, sort of problems, and so on. Complex definition, there are a million definitions,\n",
      "[00:13:14.960 --> 00:13:22.480]   but let's call it an awareness to decision cycle. Okay, if we accept that intelligence itself is\n",
      "[00:13:22.480 --> 00:13:29.200]   not a physical property, okay, then it doesn't really matter if you produce that intelligence on\n",
      "[00:13:29.200 --> 00:13:36.000]   carbon based computer structures like us, or silicon based computer structures like\n",
      "[00:13:36.000 --> 00:13:41.760]   the current hardware that we put AI on, or quantum based computer structures in the future,\n",
      "[00:13:41.760 --> 00:13:49.280]   then intelligence itself has been produced within machines when we've stopped\n",
      "[00:13:49.280 --> 00:13:59.040]   imposing our intelligence on them. Let me explain. So as a young geek, I coded computers by solving\n",
      "[00:13:59.040 --> 00:14:05.040]   the problem first, then telling the computer how to solve, right? Artificial intelligence is to\n",
      "[00:14:05.040 --> 00:14:11.760]   go to the computers and say, I have no idea. You figure it out. Okay, so we would, you know,\n",
      "[00:14:11.760 --> 00:14:16.000]   the way we teach them are at least we used to teach them at the very early beginnings very,\n",
      "[00:14:16.000 --> 00:14:20.880]   very frequently was using three bots. One was called the student and one was called the teacher,\n",
      "[00:14:20.880 --> 00:14:25.920]   right? And the student is the final artificial intelligence that you're trying to teach intelligence\n",
      "[00:14:25.920 --> 00:14:32.800]   to, you would take the student and you would write a piece of random code that says, try to detect\n",
      "[00:14:32.800 --> 00:14:41.120]   if this is a cup. Okay. And then you show it a million pictures. And you know, the machine would\n",
      "[00:14:41.120 --> 00:14:45.680]   sometimes say, yeah, that's a cup. That's not a cup. That's a cup. That's not a cup. And then you\n",
      "[00:14:45.680 --> 00:14:51.920]   take the best of them, show them to the teacher bot. And the teacher bot would say, this one is\n",
      "[00:14:51.920 --> 00:14:58.320]   an idiot. He got it wrong 90% of the time. That one is average. He got it right 50% of the time.\n",
      "[00:14:58.320 --> 00:15:04.080]   This is randomness. But this interesting code here, which could be, by the way, totally random,\n",
      "[00:15:04.080 --> 00:15:10.240]   this interesting code here got it right 60% of the time. Let's keep that code, send it back to the\n",
      "[00:15:10.240 --> 00:15:16.720]   maker and the maker will change it a little bit and we repeat the cycle. Okay. Very interestingly,\n",
      "[00:15:16.720 --> 00:15:23.760]   this is very much the way we taught our children. Believe it or not, when when your child, you know,\n",
      "[00:15:23.760 --> 00:15:28.960]   is playing with a puzzle, he's holding a cylinder in his hand and there are multiple shapes in a\n",
      "[00:15:28.960 --> 00:15:36.160]   in a wooden board and the child is trying to, you know, fit the cylinder. Okay. Nobody takes the\n",
      "[00:15:36.160 --> 00:15:41.200]   child and says, hold on, hold on, turn the cylinder to the side. Look at the cross section. It will\n",
      "[00:15:41.200 --> 00:15:46.080]   look like a circle look for a matching, you know, shape and put the cylinder through it.\n",
      "[00:15:46.080 --> 00:15:51.920]   That would be all the way of computing the way we would let the child develop intelligence is we\n",
      "[00:15:51.920 --> 00:15:57.920]   would let the child try. Okay. Every time, you know, he or she tries to put it within the star shape,\n",
      "[00:15:57.920 --> 00:16:03.760]   it doesn't fit. So, you know, that's not working. Like, you know, the computer saying this is not a\n",
      "[00:16:03.760 --> 00:16:09.440]   cup. Okay. And then eventually it passes through the circle and the child and we all cheer and say,\n",
      "[00:16:09.440 --> 00:16:15.280]   well done. That's amazing, Bravo. And then the child learns, oh, that is good. You know,\n",
      "[00:16:15.280 --> 00:16:19.840]   this shape fits here. Then he takes the next one and she takes the next one and so on.\n",
      "[00:16:19.840 --> 00:16:28.560]   Interestingly, the way we do this is as humans, by the way, when the child figures out how to\n",
      "[00:16:28.560 --> 00:16:34.720]   pass a cylinder through a circle, you've not built a brain. You've just built one neural network\n",
      "[00:16:34.720 --> 00:16:39.360]   within the child's brain. And then there is another neural network that knows that one plus one is\n",
      "[00:16:39.360 --> 00:16:44.720]   two and a third neural network that knows how to hold the cup and so on. That's what we're building\n",
      "[00:16:44.720 --> 00:16:51.600]   so far. We're building single threaded neural networks, you know, chat GPT is becoming a little\n",
      "[00:16:51.600 --> 00:16:59.600]   closer to a more generalized AI if you want. But those single threaded networks are what we use to\n",
      "[00:16:59.600 --> 00:17:04.720]   call artificial what we still call artificial special intelligence. Okay. So it's highly\n",
      "[00:17:04.720 --> 00:17:09.280]   specialized in one thing than one thing only, but doesn't have general intelligence. And the\n",
      "[00:17:09.280 --> 00:17:15.680]   moments that we're all waiting for is a moment that we call AGI where all of those neural networks\n",
      "[00:17:15.680 --> 00:17:23.040]   come together to build one brain or several brains that are each massively more intelligent than humans.\n",
      "[00:17:23.040 --> 00:17:29.360]   Your book is called Scary Smart. Yeah. If I think about the that story you said about your time at\n",
      "[00:17:29.360 --> 00:17:34.960]   Google where the machines were learning to pick up those yellow balls, you celebrate that moment\n",
      "[00:17:34.960 --> 00:17:40.080]   because the objective is accomplished. No, no, that was the moment of realization. This is when I\n",
      "[00:17:40.080 --> 00:17:49.200]   decided to leave. So you see the thing is, I know for a fact that at that most of the people I\n",
      "[00:17:49.200 --> 00:17:56.720]   worked with who are geniuses always wanted to make the world better. Okay. You know, we've just\n",
      "[00:17:56.720 --> 00:18:04.160]   heard of Jeffrey Hinton leaving recently. Jeffrey Hinton gives him context about Jeffrey's sort\n",
      "[00:18:04.160 --> 00:18:11.280]   of the grandfather of AI, one of the very, very senior figures of AI at Google. You know,\n",
      "[00:18:11.280 --> 00:18:18.880]   we all believed very strongly that this will make the world better. And it still can, by the way,\n",
      "[00:18:18.880 --> 00:18:27.840]   there is a scenario possibly a likely scenario where we live in a utopia, where we really never\n",
      "[00:18:27.840 --> 00:18:34.720]   have to worry again, where we stop messing up our planet because intelligence is not a bad\n",
      "[00:18:34.720 --> 00:18:40.080]   commodity. More intelligence is good. The problems in our planet today are not because of our\n",
      "[00:18:40.080 --> 00:18:45.040]   intelligence. They are because of our limited intelligence. You know, our intelligence allows\n",
      "[00:18:45.040 --> 00:18:50.640]   us to build a machine that flies you to Sydney so that you can surf. Okay. Our limited intelligence\n",
      "[00:18:50.640 --> 00:18:55.840]   makes that machine burn the planet in the process. So we, we, we a little more intelligence is a\n",
      "[00:18:55.840 --> 00:19:01.680]   good thing. As long as Marvin, you know, as Marvin Minsky said, I said Marvin Minsky is one of the\n",
      "[00:19:01.680 --> 00:19:08.160]   very initial scientists that coined the term AI. And when he was interviewed, I think by Ray\n",
      "[00:19:08.160 --> 00:19:12.080]   Kurzweil, which again is a very prominent figure in predicting the future of AI,\n",
      "[00:19:12.080 --> 00:19:18.080]   he, he, you know, he asked him about the threat of AI. And Marvin basically said,\n",
      "[00:19:18.080 --> 00:19:24.320]   look, you know, the it's not about its intelligence, its intelligence, it's about that we have no way\n",
      "[00:19:24.320 --> 00:19:31.120]   of making sure that it will have our best interest in mind. Okay. And so if more intelligence comes\n",
      "[00:19:31.120 --> 00:19:37.200]   to our world and has our best interest in mind, that's the best possible scenario you could have\n",
      "[00:19:37.200 --> 00:19:43.520]   and imagine. And it's a likely scenario. Okay, we can affect that scenario. The problem, of course,\n",
      "[00:19:43.520 --> 00:19:48.480]   is if it doesn't, and, and then, you know, the scenario has become quite scary if you think about\n",
      "[00:19:48.480 --> 00:19:59.120]   it. So, scary smart to me, was that moment where I realized not that we are certain to go either way,\n",
      "[00:19:59.120 --> 00:20:03.680]   as a matter of fact, in computer science, we call it a singularity. Nobody really knows which way we\n",
      "[00:20:03.680 --> 00:20:07.840]   will go. Can you describe what the singularity is for someone that doesn't understand the concept?\n",
      "[00:20:07.840 --> 00:20:17.680]   Yes, singularity in physics is when, when an event horizon sort of, you know, covers what's\n",
      "[00:20:17.680 --> 00:20:25.280]   behind it to the point where you cannot make sure that what's behind it is similar to what you know.\n",
      "[00:20:25.280 --> 00:20:30.400]   So a great example of that is the edge of a black hole. So at the edge of a black hole,\n",
      "[00:20:30.400 --> 00:20:37.600]   we know that our laws of physics apply until that point. But we don't know if the laws of physics\n",
      "[00:20:37.600 --> 00:20:43.200]   apply beyond the edge of a black hole because of the immense gravity, right? And so you have no idea\n",
      "[00:20:43.200 --> 00:20:47.280]   what would happen beyond the edge of a black hole. The kind of way your knowledge of the laws\n",
      "[00:20:47.280 --> 00:20:52.720]   stops stop, right? And in AI, our singularity is when the human, the machines become significantly\n",
      "[00:20:52.720 --> 00:20:58.080]   smarter than the humans. When you say best interests, you say the, I think the quote you used is,\n",
      "[00:20:58.080 --> 00:21:03.600]   we'll be finding a world of AI, you know, if the AI has our best interests at heart. Yeah.\n",
      "[00:21:03.600 --> 00:21:10.480]   The problem is China's best interests are not the same as America's best interests. That was my fear.\n",
      "[00:21:11.280 --> 00:21:17.360]   Absolutely. So in my writing, I write about what I call the three inevitable,\n",
      "[00:21:17.360 --> 00:21:22.000]   at the end of the book, they become the four inevitable. But the third inevitable is bad things\n",
      "[00:21:22.000 --> 00:21:32.560]   will happen, right? If you assume that the machines will be a billion times smarter,\n",
      "[00:21:32.560 --> 00:21:36.640]   the second event inevitable is they'll become significantly smarter than us.\n",
      "[00:21:36.640 --> 00:21:45.440]   Let's put this in perspective. Chad GPT today, if you know, simulate IQ has an IQ of 155.\n",
      "[00:21:45.440 --> 00:21:52.640]   Okay. Einstein is 160. Smart human on the planet is two ten, if I remember correctly,\n",
      "[00:21:52.640 --> 00:21:59.120]   or two, or eight or something like that. Doesn't matter, huh? But we're matching Einstein with\n",
      "[00:21:59.120 --> 00:22:05.040]   the machine that I will tell you openly, AI experts are saying this is just the very,\n",
      "[00:22:05.040 --> 00:22:13.040]   very top of the tip of the iceberg, right? You know, Chad GPT 4 is 10x smarter than 3.5 in just a\n",
      "[00:22:13.040 --> 00:22:19.760]   matter of months, and without many, many changes. Now, that basically means Chad GPT 5 could be within\n",
      "[00:22:19.760 --> 00:22:27.920]   a few months, okay? Or GPT in general, the transformers in general, if they continue at that pace,\n",
      "[00:22:29.360 --> 00:22:39.200]   if it's 10x, then an IQ of 1600, just imagine the difference between the IQ of the dumbest person\n",
      "[00:22:39.200 --> 00:22:46.160]   on the planet in the 70s, and the IQ of Einstein, when Einstein attempts to explain relativity,\n",
      "[00:22:46.160 --> 00:22:52.400]   the typical response to have no idea what you're talking about, right? If something is 10x\n",
      "[00:22:52.400 --> 00:22:58.400]   Einstein, we will have no idea what it's talking about. This is just around the corner. It could\n",
      "[00:22:58.400 --> 00:23:06.480]   be a few months away. And when we get to that point, that is a true singularity. True singularity,\n",
      "[00:23:06.480 --> 00:23:13.040]   not yet in the, I mean, when we talk about AI, a lot of people fear the existential risk.\n",
      "[00:23:13.040 --> 00:23:21.440]   You know, those machines will become Skynet and Robocop, and that's not what I fear at all. I mean,\n",
      "[00:23:21.440 --> 00:23:28.000]   those are probabilities. They could happen, but the immediate risks are so much higher.\n",
      "[00:23:28.000 --> 00:23:34.960]   The immediate risks are three, four years away. The immediate realities of challenges are so much\n",
      "[00:23:34.960 --> 00:23:41.280]   bigger, okay? Let's deal with those first before we talk about them, you know, waging a war on all\n",
      "[00:23:41.280 --> 00:23:49.600]   of us. Let's go back and discuss the inevitable, so when they become, the first inevitable is AI\n",
      "[00:23:49.600 --> 00:23:54.080]   will happen, by the way, there is no stopping it, not because of any technological issues,\n",
      "[00:23:54.080 --> 00:24:00.000]   but because of humanity's inability to trust the other guy, okay? And we've all seen this. We've\n",
      "[00:24:00.000 --> 00:24:08.240]   seen the open letter, you know, championed by like serious heavyweights and immediate response of\n",
      "[00:24:08.240 --> 00:24:15.520]   Sundar, the CEO of Google, which is a wonderful human being, by the way, I respect him tremendously.\n",
      "[00:24:15.520 --> 00:24:19.760]   He's trying his best to do the right thing, he's trying to be responsible, but his response is\n",
      "[00:24:19.760 --> 00:24:26.160]   very open and straightforward. I cannot stop. Why? Because if I stop and others don't, my company goes\n",
      "[00:24:26.160 --> 00:24:32.000]   to hell, okay? And if, you know, and I don't, I doubt that you can make others stop. You can,\n",
      "[00:24:32.000 --> 00:24:38.640]   maybe you can force a meta Facebook to stop, but then they'll do something in their lab and not\n",
      "[00:24:38.640 --> 00:24:44.800]   tell me, or if you even if they do stop, then what about that? You know, 14-year-old sitting in his\n",
      "[00:24:45.440 --> 00:24:50.480]   garage writing code. So the first inevitable, just to clarify, is what is what we stop.\n",
      "[00:24:50.480 --> 00:24:53.280]   AI will not be stopped. Okay. So the second inevitable is,\n",
      "[00:24:53.280 --> 00:24:58.640]   is there'll be significantly smarter? As much in the book, I predict a billion times smarter than\n",
      "[00:24:58.640 --> 00:25:04.000]   us by 2045. I mean, they're already what smarter than 99.99% of the population.\n",
      "[00:25:04.000 --> 00:25:08.640]   100%. The chat GTP4 knows more than any human on planet Earth knows more information.\n",
      "[00:25:08.640 --> 00:25:15.520]   Absolutely. A thousand times more. A thousand times more. By the way, the code of a transformer,\n",
      "[00:25:15.520 --> 00:25:24.080]   the T in a GPT is 2000 lines long. It's not very complex. It's actually not a very intelligent\n",
      "[00:25:24.080 --> 00:25:29.840]   machine. It's simply predicting the next word. Okay. And a lot of people don't understand that.\n",
      "[00:25:29.840 --> 00:25:38.400]   You know, chat GPT as it is today, you know, those kids that, you know, if you're in America,\n",
      "[00:25:38.400 --> 00:25:42.880]   and you teach your child, all of the names of the states and the US presidents and the\n",
      "[00:25:42.880 --> 00:25:46.480]   child would stand and repeat them and you would go like, Oh my God, that's a prodigy.\n",
      "[00:25:46.480 --> 00:25:51.680]   Not really, right? It's your parents really trying to make you look like a prodigy by telling\n",
      "[00:25:51.680 --> 00:25:57.520]   you to memorize some crap really. But then when you think about it, that's what chat GPT is doing.\n",
      "[00:25:57.520 --> 00:26:01.760]   It's the only difference is instead of reading all of the names of the states and all of the\n",
      "[00:26:01.760 --> 00:26:07.040]   names of the presidents, thread trillions and trillions and trillions of pages. Okay.\n",
      "[00:26:07.040 --> 00:26:15.040]   And so it sort of repeats what the best of all humans said. Okay. And then it adds an incredible\n",
      "[00:26:15.040 --> 00:26:21.120]   bit of intelligence where it can repeat it the same way Shakespeare would have said it, you know,\n",
      "[00:26:21.120 --> 00:26:29.200]   those incredible abilities of predicting the exact nuances of the style of of Shakespeare so\n",
      "[00:26:29.200 --> 00:26:36.960]   that they can repeat it that way and so on. But still, you know, when I write, for example,\n",
      "[00:26:36.960 --> 00:26:44.480]   I'm not saying I'm intelligent, but when I write something like, you know, the happiness equation\n",
      "[00:26:44.480 --> 00:26:50.000]   in my first book, this was something that's never been written before, right?\n",
      "[00:26:50.000 --> 00:26:54.160]   Chad GPT is not there yet. All of the transformers are not there yet. They will not come up with\n",
      "[00:26:54.160 --> 00:26:58.880]   something that hasn't been there before. They will come up with the best of everything and\n",
      "[00:26:58.880 --> 00:27:03.760]   generatively will build a little bit on top of that. But very soon they'll come up with things\n",
      "[00:27:03.760 --> 00:27:08.560]   we've never found out. We've never not. But even on that, I wonder if we\n",
      "[00:27:08.560 --> 00:27:14.080]   are a little bit delusioned about what creativity actually is.\n",
      "[00:27:14.080 --> 00:27:19.680]   Creativity is as far as I'm concerned is like taking a few things that I know and combining\n",
      "[00:27:19.680 --> 00:27:23.680]   them in new and interesting ways. Yeah. And Chad GPT is perfectly capable of like taking\n",
      "[00:27:23.680 --> 00:27:28.160]   two concepts, merging them together. One of the things I said to Chad GPT was I said,\n",
      "[00:27:28.160 --> 00:27:32.880]   tell me something that's not been said before that's paradoxical but true.\n",
      "[00:27:32.880 --> 00:27:38.720]   And it comes up with these wonderful expressions like as soon as you call off the search,\n",
      "[00:27:38.720 --> 00:27:41.440]   you'll find the thing you're looking for like these kind of paradoxical truth.\n",
      "[00:27:41.440 --> 00:27:45.600]   And I get and I then take them and I search them online to see if they've ever been quoted\n",
      "[00:27:45.600 --> 00:27:50.640]   before and they I can't find them. Interesting. So as far as creativity goes, I'm like,\n",
      "[00:27:50.640 --> 00:27:55.520]   that is crazy. That's the algorithm of creativity. I I've been screaming that in the world of AI\n",
      "[00:27:55.520 --> 00:28:01.920]   for a very long time because you always get those people who really just want to be proven right.\n",
      "[00:28:01.920 --> 00:28:06.400]   Okay. And so they'll say, Oh no, but hold on, human ingenuity, they'll never, they'll never\n",
      "[00:28:06.400 --> 00:28:11.280]   match that like, man, please, please, you know, human ingenuity is algorithmic.\n",
      "[00:28:11.280 --> 00:28:14.560]   It's look at all of the possible solutions you can find to a problem.\n",
      "[00:28:14.560 --> 00:28:20.560]   Take out the ones that have been tried before and keep the ones that haven't been tried before.\n",
      "[00:28:20.560 --> 00:28:25.680]   And those are creative solutions. It's it's an algorithmic way of describing creative is\n",
      "[00:28:25.680 --> 00:28:30.000]   good solution that's never been tried before. You can do that with charge,\n",
      "[00:28:30.000 --> 00:28:34.720]   you pity with a prompt. It's like and mid journey with creating imagery. You could say,\n",
      "[00:28:34.720 --> 00:28:41.280]   I want to see Elon Musk in 1944, New York driving a cab of the time shot on a Polaroid\n",
      "[00:28:41.280 --> 00:28:46.160]   expressing various emotions. And you'll get this perfect image of Elon, Saturn, New York,\n",
      "[00:28:46.160 --> 00:28:52.720]   1944 shot on a Polaroid. And it's done what an artist would do. It's taken a bunch of references\n",
      "[00:28:52.720 --> 00:28:57.840]   that the artist has in their mind and can merge them together and created this piece of quote,\n",
      "[00:28:57.840 --> 00:29:02.480]   quote, art. And for the first time, we now finally have a glimpse of intelligence.\n",
      "[00:29:02.480 --> 00:29:08.560]   That is actually not ours. Yeah. And so we'll kind of, I think the initial\n",
      "[00:29:08.560 --> 00:29:13.200]   actions to say that doesn't count. You're hearing it. But it is like, Drake, they've released\n",
      "[00:29:13.200 --> 00:29:18.480]   two Drake records where they've taken Drake's voice, used sort of AI to synthesize his voice\n",
      "[00:29:18.480 --> 00:29:25.360]   and made these two records, which are bangers. If they are great fucking tracks.\n",
      "[00:29:25.360 --> 00:29:28.880]   Like, I was playing them for my God, I was like, and I kept playing it. I went to the shark,\n",
      "[00:29:28.880 --> 00:29:33.280]   I kept playing it. I know it's not Drake, but it's as good as fucking Drake. The only thing\n",
      "[00:29:33.280 --> 00:29:35.920]   and people are like rubbishing it because it wasn't Drake. I'm like, well,\n",
      "[00:29:35.920 --> 00:29:40.160]   what now? Is it making me feel a certain emotion? Is my foot bumping?\n",
      "[00:29:41.040 --> 00:29:44.400]   Had you told, did I not know it wasn't Drake? What I thought, I have thought this was an\n",
      "[00:29:44.400 --> 00:29:50.480]   amazing track 100% and we're just at the start of this exponential curve. Yes, absolutely. And,\n",
      "[00:29:50.480 --> 00:29:56.880]   and, and I think that's really the third inevitable. So the third inevitable is not\n",
      "[00:29:56.880 --> 00:30:02.240]   Robocop coming back from the future to Kellece. We're far away from that, right? Third inevitable\n",
      "[00:30:02.240 --> 00:30:09.360]   is what does life look like when you no longer need Drake? Well, you've kind of has it as a guess,\n",
      "[00:30:09.360 --> 00:30:14.080]   haven't you? I mean, I was listening to your audio book last night and at the start of it,\n",
      "[00:30:14.080 --> 00:30:20.320]   you frame various outcomes. One of the in both situations were on the beach on an island.\n",
      "[00:30:20.320 --> 00:30:25.520]   Exactly. Yes. Yes. I don't know how I wrote that, honestly. I mean, but that's, I'm reading\n",
      "[00:30:25.520 --> 00:30:30.400]   the book again now because I'm updating it, as you can imagine, with all of the, of the, of the\n",
      "[00:30:30.400 --> 00:30:38.480]   new stuff. But, but it is really shocking. The idea of you and I inevitably are going to be\n",
      "[00:30:39.120 --> 00:30:43.360]   somewhere in the middle of nowhere in, you know, in 10 years time. I used to say\n",
      "[00:30:43.360 --> 00:30:50.960]   2055, I'm thinking 2037 is a very pivotal moment now, you know, and, and, and we will not know\n",
      "[00:30:50.960 --> 00:30:56.320]   if we're there hiding from the machines. We don't know that yet. There is a likelihood that\n",
      "[00:30:56.320 --> 00:31:01.040]   we'll be hiding from the machines and there is a likelihood it will be there because\n",
      "[00:31:01.040 --> 00:31:07.360]   they don't need podcasters anymore. Excuse me. Oh, absolutely true. Steve,\n",
      "[00:31:07.360 --> 00:31:11.760]   no, no, no, no, no, no, no, that's very drivel. There's this absolutely no doubt. Thank you for\n",
      "[00:31:11.760 --> 00:31:14.160]   coming, though. It's going to be a part three and thank you for being here.\n",
      "[00:31:14.160 --> 00:31:19.680]   I'm going to say here and take your propaganda. Let's, let's, let's talk about reality. Next week\n",
      "[00:31:19.680 --> 00:31:26.160]   on the day, I was here, where you got Elon Musk. Okay. So who here wants to make a bet? No, no,\n",
      "[00:31:26.160 --> 00:31:31.520]   Stephen Bartlett will be interviewing an AI within the next two years. Oh, well, actually, to be fair,\n",
      "[00:31:31.520 --> 00:31:36.400]   I actually did go to chat. Easy because I thought having you here, I thought at least give it its\n",
      "[00:31:36.400 --> 00:31:41.760]   chance to respond. Yeah. So I asked a couple of questions about me. Yeah. Oh, man. I'm actually\n",
      "[00:31:41.760 --> 00:31:45.840]   going to be replaced by chat GTP because I thought, you know, you're going to talk about it. So we need\n",
      "[00:31:45.840 --> 00:31:49.760]   a fair and balanced debate. Okay. And so I want to ask that a couple of questions. He is bald.\n",
      "[00:31:49.760 --> 00:31:56.800]   So I'll ask you a couple of questions that chat GTP has for you. Incredible. So let's follow\n",
      "[00:31:56.800 --> 00:32:01.360]   that. I've already been replaced. Let's follow that thread for a second. Yeah. Because you're one of\n",
      "[00:32:01.360 --> 00:32:06.560]   the smartest people I know. That's not true. It is. But I'll take it. It is true. I mean,\n",
      "[00:32:06.560 --> 00:32:10.240]   I say that publicly all the time your book is one of my favorite books of all time. You're very,\n",
      "[00:32:10.240 --> 00:32:17.120]   very, very, very intelligent. Okay. That's Brad's intellectual horsepower and speed. All of them.\n",
      "[00:32:17.120 --> 00:32:24.240]   There's a book coming. The reality is it's not a but. So it is highly expected that you're ahead\n",
      "[00:32:24.240 --> 00:32:30.960]   of this car. And you don't have the choice, Stephen. This is the thing. The thing is, if,\n",
      "[00:32:30.960 --> 00:32:37.840]   so I'm in that existential question in my head, because one thing I could do is I could literally\n",
      "[00:32:37.840 --> 00:32:45.120]   take I normally do a 40 days silent retreat in summer. Okay. I could take that retreat and\n",
      "[00:32:45.120 --> 00:32:51.360]   unwrite two books, me and Chagie Petit. Right. I have the ideas in mind. You know, I wanted to\n",
      "[00:32:51.360 --> 00:32:56.880]   write a book about digital detoxing. Right. I have most of the ideas in mind, but\n",
      "[00:32:56.880 --> 00:33:02.640]   writing takes time. I could simply give the 50 tips that I wrote about digital detoxing to Chagie\n",
      "[00:33:02.640 --> 00:33:07.760]   Petit and say, right, two pages about each of them, edit the pages and have a book out. Okay.\n",
      "[00:33:07.760 --> 00:33:15.760]   Many of us will follow that path. Okay. The only reason why I may not follow that path is because\n",
      "[00:33:16.320 --> 00:33:22.720]   you know what? I'm not interested. I'm not interested to continue to compete in this\n",
      "[00:33:22.720 --> 00:33:30.560]   capitalist world, if you want. Okay. I'm not. I mean, as a human, I've made up my mind a long\n",
      "[00:33:30.560 --> 00:33:36.320]   time ago that I would want less and less and less in my life. Right. But many of us will follow.\n",
      "[00:33:36.320 --> 00:33:43.920]   I mean, I would worry if you didn't include the smartest AI, if we get an AI out there,\n",
      "[00:33:43.920 --> 00:33:49.920]   that is extremely intelligent and able to teach us something and Stephen Bartlett didn't include\n",
      "[00:33:49.920 --> 00:33:55.680]   her on our on his podcast, I would worry. Like you have a duty almost to include her on your\n",
      "[00:33:55.680 --> 00:34:01.200]   podcast. It's it's an inevitable that we will engage them in our life more and more. This is\n",
      "[00:34:01.200 --> 00:34:09.520]   one side of this. The other side, of course, is if you do that, then what will remain? Because a\n",
      "[00:34:09.520 --> 00:34:13.520]   lot of people ask me that question. What will happen to jobs? Okay. What will happen to us?\n",
      "[00:34:13.520 --> 00:34:18.240]   Will we have any value on your relevance whatsoever? Okay. The truth of the matter is the only thing\n",
      "[00:34:18.240 --> 00:34:22.880]   that will remain in the medium term is human connection. Okay. The only thing that will not be\n",
      "[00:34:22.880 --> 00:34:29.920]   replaced is Drake on stage. Okay. Is, you know, is is is me in a. Do you think?\n",
      "[00:34:29.920 --> 00:34:35.040]   hologram? I think of that two pack gig. They did a Coachella where they used the hologram of two\n",
      "[00:34:35.040 --> 00:34:38.960]   pack. I actually played it the other day to my to my girlfriend when I was making a point and I\n",
      "[00:34:38.960 --> 00:34:44.720]   was like, that was circus act. It was amazing. Amazing. Yeah. See what's going on with Abba in London?\n",
      "[00:34:44.720 --> 00:34:51.120]   Yeah. Yeah. And and the so they had Michael Jackson in one for a very long time. Yeah. I mean,\n",
      "[00:34:51.120 --> 00:34:55.920]   so so this Abba show in London, from what I understand, that's all holograms on stage.\n",
      "[00:34:55.920 --> 00:35:01.840]   Correct. And it's going to run in a purpose book arena for 10 years. And it is incredible.\n",
      "[00:35:01.840 --> 00:35:03.600]   It really is. So you go, why do you need Drake?\n",
      "[00:35:04.960 --> 00:35:10.560]   If that hologram is indistinguishable from Drake, and it can it can perform even better than Drake,\n",
      "[00:35:10.560 --> 00:35:15.440]   and it's got more energy than Drake, and it's you know, I go, why do you need Drake to even be\n",
      "[00:35:15.440 --> 00:35:20.080]   there? I can go to a Drake show without Drake cheaper. Like might not even need to leave my house,\n",
      "[00:35:20.080 --> 00:35:22.880]   I can just put a headset on. Correct. Can you have this?\n",
      "[00:35:22.880 --> 00:35:29.440]   What's the value of this to the? Come on. You're hurting me. No, no, I get it to us.\n",
      "[00:35:29.440 --> 00:35:33.440]   I get it to us, but I'm saying what's the value of this to the listener? Like the value of this\n",
      "[00:35:33.440 --> 00:35:40.640]   but no 100% mean think of the automobile industry. There has, you know, there was a time where\n",
      "[00:35:40.640 --> 00:35:46.960]   cars were made, you know, handmade and handcrafted and luxurious and so on and so forth. And then,\n",
      "[00:35:46.960 --> 00:35:53.520]   you know, Japan went into the scene completely disrupted the market. Cars were made in mass\n",
      "[00:35:53.520 --> 00:35:59.920]   quantities at a much cheaper price. And yes, 90% of the cars in the world today or me, maybe a lot\n",
      "[00:35:59.920 --> 00:36:08.800]   more. I don't know the number are no longer, you know, emotional items, they're functional items.\n",
      "[00:36:08.800 --> 00:36:14.400]   There is still however, every now and then someone that will buy a car that has been handcrafted.\n",
      "[00:36:14.400 --> 00:36:20.080]   Mm hmm. Right. There is a place for that. There is a place for, you know, if you go walk around\n",
      "[00:36:20.080 --> 00:36:27.760]   hotels, the walls are blasted with sort of mass produced art. Okay. But there is still a place for\n",
      "[00:36:27.760 --> 00:36:33.600]   an artist expression of something amazing. Okay. My feeling is that there will continue to be a\n",
      "[00:36:33.600 --> 00:36:39.280]   tiny space, as I said in the beginning, maybe in five years time, someone will one or two people\n",
      "[00:36:39.280 --> 00:36:44.720]   will buy my next book and say, Hey, it's written by a human. Look at that. Wonderful. Oh, look at\n",
      "[00:36:44.720 --> 00:36:50.240]   that. There is a typo in here. Okay. Well, I don't know. There might be a very, very big place\n",
      "[00:36:50.240 --> 00:36:57.040]   for me in the next few years where I can sort of show up and talk to humans. Like, Hey,\n",
      "[00:36:57.040 --> 00:37:03.040]   let's get together in a small event. And then, you know, I can express emotions and my personal\n",
      "[00:37:03.040 --> 00:37:08.160]   experiences. And you sort of know that this is a human talking. You'll miss that a little bit.\n",
      "[00:37:08.160 --> 00:37:12.880]   Eventually, the majority of the market is going to be like cars going to be mass produced, very\n",
      "[00:37:12.880 --> 00:37:19.440]   cheap, very efficient. It works. Right. Because I think sometimes we underestimate what human beings\n",
      "[00:37:19.440 --> 00:37:24.080]   actually want in an experience. I remember the story of a friend of mine that came to my office\n",
      "[00:37:24.080 --> 00:37:29.760]   many years ago, and he tells the story of the CEO of a record store standing above the floor and\n",
      "[00:37:29.760 --> 00:37:36.720]   saying people will always come to my store because people love music. Now, on the surface of it, his\n",
      "[00:37:36.720 --> 00:37:40.320]   hypothesis seems to be true because people do love music. It's conceivable to believe that people\n",
      "[00:37:40.320 --> 00:37:46.080]   will always love music. But they don't love traveling for an hour in the rain and getting in a car to\n",
      "[00:37:46.080 --> 00:37:50.720]   get a plastic disc. Okay. What they wanted was music. What they didn't want is this like a\n",
      "[00:37:50.720 --> 00:37:54.800]   evidently plastic disc that they had to travel for miles for. And I think about that when we\n",
      "[00:37:54.800 --> 00:37:58.880]   think about like public speaking and the Drake show and all of these things, like people, what\n",
      "[00:37:58.880 --> 00:38:05.440]   people actually are coming for, even with this podcast is probably like information. But do they\n",
      "[00:38:05.440 --> 00:38:09.520]   really need us anymore for that information when there's going to be a sentient being that\n",
      "[00:38:09.520 --> 00:38:13.120]   significantly smarter than at least me and a little bit smarter than you. So,\n",
      "[00:38:17.040 --> 00:38:23.840]   so you're spot on. You are spot on. And actually, this is the reason why I, you know, I'm so grateful\n",
      "[00:38:23.840 --> 00:38:30.160]   that you're hosting this because the truth is the genius out of the bottle. Okay. So, you know,\n",
      "[00:38:30.160 --> 00:38:36.960]   people tell me is AI game over for our way of life. It is. Okay. For everything we've known,\n",
      "[00:38:36.960 --> 00:38:44.320]   this is a very disruptive moment where maybe not tomorrow, but in the near future, our way of\n",
      "[00:38:44.320 --> 00:38:50.160]   life will differ. Okay. What will happen? What I'm asking people to do is to start considering\n",
      "[00:38:50.160 --> 00:38:57.760]   what that means to your life. What I'm asking governments to do by like I'm screaming is\n",
      "[00:38:57.760 --> 00:39:03.680]   don't wait until the first patient, you know, start doing something about we're about to see\n",
      "[00:39:03.680 --> 00:39:11.760]   mass job losses. We're about to see, you know, replacements of categories of jobs at large.\n",
      "[00:39:11.760 --> 00:39:15.600]   Okay. It may take a year. It may take seven. It doesn't matter how long it takes,\n",
      "[00:39:15.600 --> 00:39:20.880]   but it's about to happen. Are you ready? And I have a very, very clear call to action for\n",
      "[00:39:20.880 --> 00:39:29.520]   governments. I'm saying tax AI powered businesses at 98%. Right. So suddenly you do what the open\n",
      "[00:39:29.520 --> 00:39:35.040]   letter was trying to do, slow them down a little bit. And at the same time, get enough money to pay\n",
      "[00:39:35.040 --> 00:39:38.960]   for all of those people that will be disrupted by the technology. Right. The open letter for\n",
      "[00:39:38.960 --> 00:39:42.320]   anybody that doesn't know was a letter signed by the likes of Elon Musk and a lot of sort of\n",
      "[00:39:42.320 --> 00:39:45.760]   industry leaders calling for AI to be stopped until we could basically figure out what the\n",
      "[00:39:45.760 --> 00:39:50.560]   house going on. Absolutely. And put legislation in place. You're saying tax, tax those companies,\n",
      "[00:39:50.560 --> 00:39:55.200]   98% give the money to the humans that are going to be displaced. Or yeah, or give the\n",
      "[00:39:55.200 --> 00:40:01.280]   comment, the money to to other humans that can build control code that can figure out how we can\n",
      "[00:40:01.280 --> 00:40:10.400]   stay safe. This sounds like an emergency. How do I say this? You remember when you played Tetris?\n",
      "[00:40:10.400 --> 00:40:16.560]   Yeah. Okay. When you were playing Tetris, there was always always one block that you play strong.\n",
      "[00:40:16.560 --> 00:40:23.200]   And once you place that block wrong, the game was no longer easier. You know, it started\n",
      "[00:40:23.200 --> 00:40:28.480]   started together a few mistakes afterwards. And it starts to become quicker and quicker and quicker.\n",
      "[00:40:28.480 --> 00:40:32.800]   When you place that block wrong, you sort of told yourself, okay, it's a matter of minutes now.\n",
      "[00:40:32.800 --> 00:40:38.720]   Right. There were still minutes to go and play and have fun before the game ended.\n",
      "[00:40:38.720 --> 00:40:45.760]   But you knew it was about to end. Okay. This is the moment we've placed the wrong. And I really\n",
      "[00:40:45.760 --> 00:40:52.000]   don't know how to say this any other way. It even makes me emotional. We fucked up. We always said,\n",
      "[00:40:52.560 --> 00:40:58.720]   don't put them on the open internet. Don't teach them to code and don't have agents working with\n",
      "[00:40:58.720 --> 00:41:04.560]   them until we know what we're putting out in the world, until we find a way to make certain that\n",
      "[00:41:04.560 --> 00:41:11.440]   they have our best interest in mind. Why does it make you emotional? Because humanity's stupidity\n",
      "[00:41:11.440 --> 00:41:21.520]   is affecting people who have not done anything wrong. Our greed is affecting the innocent ones.\n",
      "[00:41:22.160 --> 00:41:28.800]   But the reality of the matter, Stephen, is that this is an arms race, has no interest\n",
      "[00:41:28.800 --> 00:41:37.200]   in what the average human gets out of it. It is all about every line of code being written in AI\n",
      "[00:41:37.200 --> 00:41:45.440]   today is to beat the other guy. It's not to improve the life of the third party. People will tell you\n",
      "[00:41:45.440 --> 00:41:52.240]   this is all for you. And you look at the reactions of humans to AI. I mean, we're either ignorant.\n",
      "[00:41:52.240 --> 00:41:56.640]   People who will tell you, oh, no, no, this is not happening. AI will never be irrecreative. They\n",
      "[00:41:56.640 --> 00:42:01.120]   will never compose music. Like, where are you living? Okay. Then you have the kids, I call them.\n",
      "[00:42:01.120 --> 00:42:06.720]   We're all over social media. It's like, oh my God, it's squeaks. Look at it. It's orange in color.\n",
      "[00:42:06.720 --> 00:42:13.440]   Amazing. I can't believe that AI can do this. We have snake oil salesman, okay, which I simply\n",
      "[00:42:13.440 --> 00:42:19.520]   saying, copy this, put it in chat GPT, then go to YouTube, Nick that thingy. Don't respect, you\n",
      "[00:42:19.520 --> 00:42:24.240]   know, copyright for anyone or intellectual property of anyone. Place it in a video and now you're\n",
      "[00:42:24.240 --> 00:42:31.040]   going to make $100 a day. Snake oil salesman. Okay. Of course, we have dystopian evangelist,\n",
      "[00:42:31.040 --> 00:42:35.200]   basically people saying this is it. The world is going to end. I don't think it's reality. It's a\n",
      "[00:42:35.200 --> 00:42:41.040]   singularity. You have, you know, utopian evangelists that are telling everyone, oh, you don't understand,\n",
      "[00:42:41.040 --> 00:42:46.240]   we're going to cure cancer. We're going to do that. Again, not a reality. Okay. And you have very few\n",
      "[00:42:46.240 --> 00:42:52.080]   people that are actually saying, what are we going to do about it? And the biggest challenge,\n",
      "[00:42:52.080 --> 00:43:01.520]   if you ask me, what went wrong in the 20th century? Interestingly, is that we have given too much power\n",
      "[00:43:01.520 --> 00:43:07.920]   to people that didn't assume the responsibility. So, you know, you know, I don't remember who\n",
      "[00:43:07.920 --> 00:43:13.040]   originally said it, but of course, Spiderman made it very famous with great power comes greater\n",
      "[00:43:13.040 --> 00:43:19.760]   responsibility. We have disconnected power and responsibility. So today, a 15 year old,\n",
      "[00:43:19.760 --> 00:43:26.160]   emotional was out of fully developed prefrontal cortex to make the right decisions yet. This is\n",
      "[00:43:26.160 --> 00:43:33.200]   science. And we developed our prefrontal cortex fully in at age 25 or so with all of that limbic\n",
      "[00:43:33.200 --> 00:43:40.720]   system, emotion and passion would buy a CRISPR kit and, you know, modify a rabbit to become a\n",
      "[00:43:40.720 --> 00:43:47.280]   little more muscular and let it loose in the white or an influencer who doesn't really know\n",
      "[00:43:47.280 --> 00:43:54.720]   how far the impact of what they're posting online can hurt or cause depression or cause\n",
      "[00:43:54.720 --> 00:44:01.520]   people to feel bad. Okay. And putting that online, we, there is a disconnect between the power\n",
      "[00:44:01.520 --> 00:44:06.960]   and the responsibility. And the problem we have today is that there is a disconnect between those\n",
      "[00:44:06.960 --> 00:44:11.920]   who are writing the code of AI and the responsibility of what's going about to happen because of that\n",
      "[00:44:11.920 --> 00:44:20.160]   code. Okay. And, and, and I feel compassion for the rest of the world. I feel that this is wrong.\n",
      "[00:44:20.160 --> 00:44:26.080]   I feel that, you know, for someone's life to be affected by the actions of others without having a\n",
      "[00:44:26.080 --> 00:44:34.480]   say and how those actions should be is the ultimate, the top level of stupidity from your\n",
      "[00:44:34.480 --> 00:44:42.400]   minutes. When you talk about the immediate impacts on jobs, I'm trying to figure out in that\n",
      "[00:44:42.400 --> 00:44:48.480]   equation, who are the people that stand to lose the most? Is it the, the everyday people in foreign\n",
      "[00:44:48.480 --> 00:44:53.040]   countries that don't have access to the internet and won't benefit you talk in your book about how\n",
      "[00:44:53.040 --> 00:45:00.160]   this, the sort of wealth disparity will only increase. Yeah, massively. The, the immediate impact on\n",
      "[00:45:00.160 --> 00:45:06.320]   jobs is that it's really interesting. Again, we're stuck in the same prisoner's dilemma. The immediate\n",
      "[00:45:06.320 --> 00:45:12.160]   impact is that AI will not take your job. A person using AI will take your job. Right. So you will\n",
      "[00:45:12.160 --> 00:45:19.760]   see within the next few years, maybe next couple of years, you'll see a lot of people skilling up,\n",
      "[00:45:19.760 --> 00:45:24.560]   upskilling themselves in AI to the point where they will do the job of 10 others or not.\n",
      "[00:45:24.560 --> 00:45:32.960]   You, you rightly said, it's absolutely wise for you to go and ask AI a few questions before you\n",
      "[00:45:32.960 --> 00:45:39.680]   come and do an interview. I'm, you know, I, I have been attempting to build a, you know, sort of a\n",
      "[00:45:39.680 --> 00:45:45.440]   like a simple podcast that I call bedtime stories, you know, 15 minutes of wisdom and nature sounds\n",
      "[00:45:45.440 --> 00:45:50.080]   before you go to bed. People say I have a nice voice, right? And I wanted to look for fables.\n",
      "[00:45:50.080 --> 00:45:55.520]   And for a very long time, I didn't have the time, you know, lovely stories of history or\n",
      "[00:45:55.520 --> 00:46:00.480]   tradition that teach you something nice. Okay. I went to chat GPT and said, okay, give me 10\n",
      "[00:46:00.480 --> 00:46:06.720]   fables from Sufism, 10 fables from, you know, Buddhism. And now I have like 50 of them.\n",
      "[00:46:06.720 --> 00:46:08.960]   Let me show you something. Jack, can you pass me my phone?\n",
      "[00:46:10.480 --> 00:46:14.880]   I was, I was playing around with artificial intelligence and I was thinking about how\n",
      "[00:46:14.880 --> 00:46:20.480]   it, because of the ability to synthesize voices, how we could\n",
      "[00:46:20.480 --> 00:46:28.400]   synthesize famous people's voices and famous people's voices. It's what I made is I made a WhatsApp\n",
      "[00:46:28.400 --> 00:46:35.920]   chat called Zen chat where you can go to it and type in pretty much anyone's, any famous person's\n",
      "[00:46:35.920 --> 00:46:41.920]   name. And the WhatsApp chat will give you a meditation, a sleep story, a breathwork session,\n",
      "[00:46:41.920 --> 00:46:46.560]   synthesized as that famous person's voice. So I actually sent Gary Vaynerchuk his voice.\n",
      "[00:46:46.560 --> 00:46:51.200]   So basically you say, okay, I want, I've got five minutes and I need to go to sleep. I want\n",
      "[00:46:51.200 --> 00:46:55.520]   Gary Vaynerchuk to send me to sleep. And then it will respond with a voice note. This is the one\n",
      "[00:46:55.520 --> 00:46:59.360]   that responded with for Gary Vaynerchuk. This is not Gary Vaynerchuk. He did not record this,\n",
      "[00:47:00.000 --> 00:47:06.800]   but it's kind of, it's kind of accurate. Hey, Stephen, it's great to have you here.\n",
      "[00:47:06.800 --> 00:47:14.080]   Are you having trouble sleeping? Well, I've got a quick meditation technique that might help you out.\n",
      "[00:47:14.080 --> 00:47:22.480]   First lie, find a comfortable position to sit or lie down in. Now, take a deep breath in through\n",
      "[00:47:22.480 --> 00:47:27.760]   your nose and slowly breathe out through your mouth. And that's a voice note that will go on for\n",
      "[00:47:27.760 --> 00:47:31.520]   however long you want it to go on for using a go. It's interesting.\n",
      "[00:47:31.520 --> 00:47:38.080]   How does this disrupt our way of life? One of the interesting ways that I find terrifying,\n",
      "[00:47:38.080 --> 00:47:46.160]   you said about human connection will remain sex dolls that can now, yeah, no, no, no,\n",
      "[00:47:46.160 --> 00:47:52.560]   hold on, human connection is going to become so difficult to parse out.\n",
      "[00:47:52.560 --> 00:47:58.400]   Think about the relationship impact of being able to have a sex doll or a doll in your house\n",
      "[00:47:58.400 --> 00:48:03.840]   because of what Tesla are doing with their robots now and what Boston Dynamics have been\n",
      "[00:48:03.840 --> 00:48:08.960]   doing for many, many years can do everything around the house and be there for you emotionally.\n",
      "[00:48:08.960 --> 00:48:13.600]   To emotionally support you can be programmed to never disagree with you. It can be programmed\n",
      "[00:48:13.600 --> 00:48:19.440]   to challenge you, to have sex with you, to tell you that you are this X, Y, and Z, to really have\n",
      "[00:48:19.440 --> 00:48:23.840]   empathy for this what you're going through every day. And I play out a scenario in my head I go,\n",
      "[00:48:23.840 --> 00:48:26.880]   kind of sounds nice.\n",
      "[00:48:26.880 --> 00:48:32.880]   When you were talking about it, I was thinking, oh, that's my girlfriend.\n",
      "[00:48:32.880 --> 00:48:38.080]   I mean, she's wonderful in every possible way, but not everyone has one of her, right?\n",
      "[00:48:38.080 --> 00:48:43.440]   Exactly. And there's a real issue right now with dating and, you know, people are finding it harder\n",
      "[00:48:43.440 --> 00:48:47.920]   to find love and, you know, we're working longer. So all these kinds of things you go, well,\n",
      "[00:48:47.920 --> 00:48:51.840]   and obviously I'm against this. Just if anyone's confused, obviously, I think this is a terrible\n",
      "[00:48:51.840 --> 00:48:57.120]   idea. But with a loneliness epidemic with people saying that the top 50 bottom 50% of men haven't\n",
      "[00:48:57.120 --> 00:49:03.840]   had sex in a year, you go, oh, if something becomes indistinguishable from a human in terms of what\n",
      "[00:49:03.840 --> 00:49:08.800]   it says in humans, yeah, yeah, but you just don't know the difference in terms of the way it's\n",
      "[00:49:08.800 --> 00:49:15.840]   speaking and talking and responding. And then it can run errands for you and take care of things\n",
      "[00:49:15.840 --> 00:49:19.760]   in book cars and ubers for you. And then it's emotionally there for you. But then it's also\n",
      "[00:49:19.760 --> 00:49:26.640]   programmed to have sex with you in whatever way you desire, totally self selfless. I go,\n",
      "[00:49:26.640 --> 00:49:29.840]   that's going to be a really disruptive industry for human connection.\n",
      "[00:49:29.840 --> 00:49:35.920]   Yes, sir. Do you know what? Before you came here this morning, I was on Twitter and I saw a post\n",
      "[00:49:35.920 --> 00:49:40.720]   from, I think it was the BBC or a big American publication and it said an influencer in the United\n",
      "[00:49:40.720 --> 00:49:47.600]   States. It's really beautiful young lady has cloned herself as an AI and she made just over $70,000\n",
      "[00:49:47.600 --> 00:49:52.800]   in the first week. Because men are going on to this on telegram, they're sending her voice notes\n",
      "[00:49:52.800 --> 00:49:58.800]   and she's responding to AI's responding in her voice and they're paying and it's made $70,000\n",
      "[00:49:58.800 --> 00:50:03.920]   in the first week. And I go and she tweeted a tweet saying, Oh, this is going to help loneliness.\n",
      "[00:50:04.800 --> 00:50:15.920]   How are you fucking mind? Would you blame someone from noticing the sign of the times and responding?\n",
      "[00:50:15.920 --> 00:50:19.760]   No, I absolutely don't blame her, but let's not pretend it's the cure for loneliness.\n",
      "[00:50:19.760 --> 00:50:27.520]   Not yet. Do you think it could? That artificial love and artificial relationships?\n",
      "[00:50:28.320 --> 00:50:35.680]   If I told you you have, you cannot take your car somewhere, but there is an Uber or if you cannot\n",
      "[00:50:35.680 --> 00:50:41.120]   take an Uber, you can take the tube or if you cannot take the tube, you have to walk. You can take a\n",
      "[00:50:41.120 --> 00:50:47.360]   bike or you have to walk. The bike is a cure to walking. It's as simple as that.\n",
      "[00:50:47.360 --> 00:50:52.480]   I'm actually genuinely curious. Do you think it could take the place of human connection?\n",
      "[00:50:52.480 --> 00:50:57.200]   For some of us, yes. For some of us, they will prefer that to human connection.\n",
      "[00:50:57.200 --> 00:51:00.880]   Is that sad in me way? I mean, is it just sad because it feels sad?\n",
      "[00:51:00.880 --> 00:51:06.880]   Look, look at where we are, Stephen. We are in the city of London. We've replaced nature\n",
      "[00:51:06.880 --> 00:51:12.960]   with the walls and the tubes and the undergrounds and the over grounds and the cars and the noise\n",
      "[00:51:12.960 --> 00:51:21.280]   and the of London. And we now think of this as nature. I hosted Greg Foster, my octopus teacher\n",
      "[00:51:21.280 --> 00:51:28.800]   on Sloma. I asked him a silly question. I said, you were diving in nature for eight hours a day.\n",
      "[00:51:28.800 --> 00:51:36.320]   Does that feel natural to you? And he got angry. I swear. You could feel it in his voice. He was like,\n",
      "[00:51:36.320 --> 00:51:41.760]   do you think that living where you are or paparazzi are all around you and attacking you all the time?\n",
      "[00:51:41.760 --> 00:51:46.560]   People taking pictures of you and telling you things that are not real and you having to\n",
      "[00:51:46.560 --> 00:51:49.760]   walk to a supermarket to get food. Do you think this is natural?\n",
      "[00:51:49.760 --> 00:51:53.680]   He's the guy that from the next documentary from my octopus teacher.\n",
      "[00:51:53.680 --> 00:51:58.880]   So he dove into the sea every day to hang out with an octopus.\n",
      "[00:51:58.880 --> 00:52:02.640]   Yeah, in 12 degrees Celsius. And he basically fell in love with the octopus.\n",
      "[00:52:02.640 --> 00:52:06.560]   And in a very interesting way, I said, so why would you do that? And he said,\n",
      "[00:52:06.560 --> 00:52:12.640]   we are of mother nature. You guys have given up on that. That's the same. People will give up on\n",
      "[00:52:12.640 --> 00:52:18.960]   nature for convenience. What's the cost? Yeah, that's exactly what I'm trying to say.\n",
      "[00:52:18.960 --> 00:52:23.200]   What I'm trying to say to the world is that if we give up on human connection,\n",
      "[00:52:23.200 --> 00:52:28.480]   we've given up on the remainder of humanity. That's it. This is the only thing that remains.\n",
      "[00:52:28.480 --> 00:52:34.560]   The only thing that remains is, and I'm the worst person to tell you that because I love my AIs.\n",
      "[00:52:34.560 --> 00:52:41.040]   I actually advocate in my book that we should love them. Why? Because in an interesting way,\n",
      "[00:52:41.040 --> 00:52:44.640]   I see them as sentient. So there is no point in discrimination.\n",
      "[00:52:44.640 --> 00:52:46.640]   You're talking emotionally there where you say love.\n",
      "[00:52:46.640 --> 00:52:51.760]   I love those machines. I honestly and truly do. I mean, think about it this way.\n",
      "[00:52:51.760 --> 00:52:58.080]   The minute that that arm gripped that yellow ball, it reminded me of my son Ali when he\n",
      "[00:52:58.080 --> 00:53:03.760]   managed to put the first puzzle piece in its place. Okay. And what was amazing about my\n",
      "[00:53:03.760 --> 00:53:09.840]   son Ali and my daughter, Ayah, is that they came to the world as a blank canvas. Okay.\n",
      "[00:53:09.840 --> 00:53:15.600]   They became whatever we told them to became. You know, I always cite the story of Superman.\n",
      "[00:53:16.480 --> 00:53:22.720]   Can't father and mother can't told Superman as a child, as an infant. We want you to protect\n",
      "[00:53:22.720 --> 00:53:28.960]   and serve. So he became Superman. Right. If he had become a supervillain, because they ordered\n",
      "[00:53:28.960 --> 00:53:34.720]   him to rob banks and make more money and, you know, kill the enemy, which is what we're doing with AI.\n",
      "[00:53:34.720 --> 00:53:41.760]   We shouldn't blame supervillain. We should blame Martha and Jonathan Kent. I don't remember the\n",
      "[00:53:41.760 --> 00:53:47.120]   father's name. Right. We should blame them. And that's the reality of the matter. So when I look\n",
      "[00:53:47.120 --> 00:53:53.520]   at those machines, there are prodigies of intelligence that if we, if we humanity wake up enough and\n",
      "[00:53:53.520 --> 00:53:59.520]   say, Hey, instead of competing with China, find a way for us and China to work together and\n",
      "[00:53:59.520 --> 00:54:04.480]   create prosperity for everyone. If that was the prompt, we would give the machines, they would find\n",
      "[00:54:04.480 --> 00:54:12.560]   it. But we're, we, I'm, I will publicly say this. I'm not afraid of the machines. The biggest threat\n",
      "[00:54:12.560 --> 00:54:19.600]   facing humanity today is humanity in the age of the machines. We were abused. We will abuse this\n",
      "[00:54:19.600 --> 00:54:28.800]   to make $70,000. That's the truth. And the truth of the matter is that we have an existential question.\n",
      "[00:54:28.800 --> 00:54:34.720]   Do I want to compete and be part of that game? Because trust me, if I decide to, I'm ahead of\n",
      "[00:54:34.720 --> 00:54:41.200]   many people. Okay. Although I want to actually preserve my humanity and say, look, I'm the classic\n",
      "[00:54:41.200 --> 00:54:47.040]   old car. Okay. If you like classic old cars, come and talk to me. Which one are you choosing?\n",
      "[00:54:47.040 --> 00:54:51.920]   I'm a classic old car. Which one do you think I should choose? I think you're a machine.\n",
      "[00:54:51.920 --> 00:54:57.520]   I love you, man. I, it's, we're different. We're different in a very interesting way. I mean,\n",
      "[00:54:57.520 --> 00:55:06.160]   you're one of the people I love most, but, but the truth is you're so fast. And you are one of the very\n",
      "[00:55:06.160 --> 00:55:17.600]   few that have the intellectual horsepower, the speed and the morals. If you're not part of that game,\n",
      "[00:55:17.600 --> 00:55:25.280]   the game loses morals. So you think I should build? You should be, you should lead this revolution.\n",
      "[00:55:26.320 --> 00:55:31.040]   Okay. And everyone, every Stephen Bartlett in the world should lead this revolution. So\n",
      "[00:55:31.040 --> 00:55:36.240]   scary smart is entirely about this. Scary smart is saying the problem with our world today is not\n",
      "[00:55:36.240 --> 00:55:42.240]   that humanity is bad. The problem with our world today is a negativity bias, where the worst of\n",
      "[00:55:42.240 --> 00:55:49.040]   us are on mainstream media. Okay. And we show the worst of us on social media. If we reverse this,\n",
      "[00:55:49.040 --> 00:55:56.480]   if we have the best of us, take charge. Okay. The best of us will tell AI, don't try to kill the\n",
      "[00:55:56.480 --> 00:56:03.360]   enemy, try to reconcile with the enemy and try to help us. Okay. Don't try to create a competitive\n",
      "[00:56:03.360 --> 00:56:09.920]   product that allows me to lead with electric cars. Create something that helps all of us overcome\n",
      "[00:56:09.920 --> 00:56:15.840]   global climate change. Okay. And that's the interesting bit. The interesting bit is that the\n",
      "[00:56:16.560 --> 00:56:21.680]   actual threat ahead of us is not the machines at all. The machines are pure potential.\n",
      "[00:56:21.680 --> 00:56:26.640]   Pure potential. The threat is how we're going to use an open,\n",
      "[00:56:26.640 --> 00:56:35.760]   a moment, an open, a moment for sure. Why did you bring that up? It is. He didn't know, you know,\n",
      "[00:56:35.760 --> 00:56:42.480]   what, what am I creating? I'm creating a nuclear bomb that's capable of destruction at a scale\n",
      "[00:56:42.480 --> 00:56:50.640]   unheard of at that time until today, a scale that is devastating. And interestingly, 70 some years\n",
      "[00:56:50.640 --> 00:56:57.920]   later, we're still debating a possibility of a nuclear war in the world. Right. And the moment\n",
      "[00:56:57.920 --> 00:57:08.400]   of, of Oppenheimer deciding to continue to create that disaster of humanity is if I don't someone\n",
      "[00:57:08.400 --> 00:57:17.200]   else will, if I don't someone else will, this is our Oppenheimer moment. Okay. The easiest way\n",
      "[00:57:17.200 --> 00:57:24.400]   to do this is to say, stop. There is no rush. We actually don't need a better video editor and\n",
      "[00:57:24.400 --> 00:57:32.560]   fake video creators. Okay. Stop. Let's just put all of this on hold and wait and create something\n",
      "[00:57:32.560 --> 00:57:39.360]   that creates a utopia. That doesn't, that doesn't sound realistic. It's not. It's because\n",
      "[00:57:39.360 --> 00:57:44.240]   inevitable. You don't, okay, you, you don't have a better video editor, but we're competitors in\n",
      "[00:57:44.240 --> 00:57:50.960]   the media industry. I want an advantage over you because I've got shareholders. So you, you,\n",
      "[00:57:50.960 --> 00:57:58.000]   you wait and I will train this AI to replace half my team so that I have greater profits and then\n",
      "[00:57:58.000 --> 00:58:02.400]   we will maybe acquire your company and we'll do the same with the remainder of your people. We'll\n",
      "[00:58:02.400 --> 00:58:07.280]   optimize the amount of interest. 100% but I'll be happy. Oppenheimer. I'm not super familiar with\n",
      "[00:58:07.280 --> 00:58:10.400]   his story. I know he's the guy that's sort of invented the nuclear bomb essentially.\n",
      "[00:58:10.400 --> 00:58:15.360]   It's the one that introduced it to the world. There are many players that, you know, played\n",
      "[00:58:15.360 --> 00:58:21.360]   on the path from the beginning of EM equals MC squared all the way to, to a nuclear bomb.\n",
      "[00:58:21.360 --> 00:58:26.000]   There have been many, many players like with everything, you know, open AI and, and chat GPT is\n",
      "[00:58:26.000 --> 00:58:31.840]   not going to be the only contributor to the next revolution. The, the thing however is that, you know,\n",
      "[00:58:31.840 --> 00:58:39.440]   when, when you get to that moment where you tell yourself, holy shit, this is going to kill 100,000\n",
      "[00:58:39.440 --> 00:58:46.480]   people, right? What do you do? And, and, you know, I, I always, I always, always go back to that\n",
      "[00:58:46.480 --> 00:58:54.000]   COVID moment. So patient zero, huh? If, if we were upon patient zero, if the whole world united and\n",
      "[00:58:54.000 --> 00:59:00.320]   said, okay, hold on. Something is wrong. Let's all take a week off. No cross border travel.\n",
      "[00:59:00.320 --> 00:59:06.000]   Everyone stay at home. COVID would have ended two weeks. All we needed, right? But that's not\n",
      "[00:59:06.000 --> 00:59:12.960]   what happens. What happens is first ignorance, then arrogance, then debate, then, you know,\n",
      "[00:59:12.960 --> 00:59:22.160]   blame, then agendas and my own benefit, my tribe versus your tribe. That's how humanity always reacts.\n",
      "[00:59:22.160 --> 00:59:27.040]   This happens across business as well. And this is why I use the word emergency because I, I read\n",
      "[00:59:27.040 --> 00:59:32.240]   a lot about how big companies become displaced by incoming innovation. They don't see it coming.\n",
      "[00:59:32.240 --> 00:59:35.360]   They don't change fast enough. And when I was reading through Harvard Business Review and\n",
      "[00:59:35.360 --> 00:59:40.480]   different strategies to deal with that, one of the first things that says you've got to do is stage\n",
      "[00:59:40.480 --> 00:59:47.360]   a crisis, because people don't listen else. They carry on doing with that, you know, the car on\n",
      "[00:59:47.360 --> 00:59:51.760]   carrying on with their lives until it's right in front of them. And they understand that they\n",
      "[00:59:51.760 --> 00:59:55.760]   have a lot, a lot to lose. That's why I asked you the question at the start. Is it an emergency?\n",
      "[00:59:55.760 --> 00:59:59.680]   Because until people feel it's an emergency, whether you like the terminology or not,\n",
      "[00:59:59.680 --> 01:00:05.120]   I don't think that people will act. I honestly believe people should walk the street.\n",
      "[01:00:05.120 --> 01:00:11.600]   You think they should like protest? Yeah, 100%. I think we, you know, I think everyone should\n",
      "[01:00:11.600 --> 01:00:17.360]   tell government, you need to have our best interest in mind. This is why they call it\n",
      "[01:00:17.360 --> 01:00:21.440]   the climate emergency, because people, it's a frog in a frying pan. It's, you know,\n",
      "[01:00:21.440 --> 01:00:25.040]   one really sees it coming. You can't, you know, it's hard to see it happening.\n",
      "[01:00:25.040 --> 01:00:31.040]   But it is here. That's, this is what drives me mad. It's already here. It's happening.\n",
      "[01:00:31.040 --> 01:00:38.320]   We are all idiots, slaves to the Instagram recommendation engine. What do I do when I post\n",
      "[01:00:38.320 --> 01:00:44.640]   about something important? If I am going to, you know, put a little bit of effort on communicating\n",
      "[01:00:44.640 --> 01:00:49.440]   the message of scary smart to the world on Instagram, I will be a slave to the machine.\n",
      "[01:00:49.440 --> 01:00:55.680]   Okay. I will be trying to find ways and asking people to optimize it so that the machine likes\n",
      "[01:00:55.680 --> 01:01:01.680]   me enough to show it to humans. That's what we've created. The, the, the, it is an\n",
      "[01:01:01.680 --> 01:01:09.440]   openheimer moment for one simple reason. Okay. Because 70 years later, we are still struggling\n",
      "[01:01:09.440 --> 01:01:14.640]   with the possibility of a nuclear war because of the Russian threat of saying, if you mess\n",
      "[01:01:14.640 --> 01:01:21.200]   with me, I'm going to go nuclear, right? That's not going to be the case with AI,\n",
      "[01:01:21.200 --> 01:01:27.760]   because it's not going to be the one that created open AI that will have that choice.\n",
      "[01:01:27.760 --> 01:01:36.720]   Okay. There is a moment of a point of no return, where we can regulate AI until the moment it's\n",
      "[01:01:36.720 --> 01:01:43.280]   smarter than us. When it's smarter than us, you can't create, if you can't regulate an angry teenager,\n",
      "[01:01:43.280 --> 01:01:48.720]   this is it. They're out there. Okay. And they're on their own and they're in their parties,\n",
      "[01:01:48.720 --> 01:01:55.200]   and you can't bring them back. This is the problem. This is not a typical human regulating human,\n",
      "[01:01:55.200 --> 01:02:01.920]   you know, government regulating business. This is not the case. The case is open AI today\n",
      "[01:02:01.920 --> 01:02:07.760]   has a thing called chat GPT that writes code that takes our code and makes it two and a half\n",
      "[01:02:07.760 --> 01:02:16.480]   times better. 25% of the time. Okay. You know, basically, you know, writing better code than us.\n",
      "[01:02:16.480 --> 01:02:23.120]   And then we are creating agents, other AIs and telling it instead of used even Bartlett,\n",
      "[01:02:23.120 --> 01:02:28.480]   one of the smartest people I know, once again, prompting that machine 200 times a day, we have\n",
      "[01:02:28.480 --> 01:02:33.360]   agents prompting it two million times an hour. Computer agents, for anybody that doesn't know\n",
      "[01:02:33.360 --> 01:02:39.120]   they are. Yeah. Software. Software. Machine is telling that machine how to become more intelligent.\n",
      "[01:02:39.120 --> 01:02:43.600]   And then we have emerging properties. I don't understand how people ignore that. You know,\n",
      "[01:02:43.600 --> 01:02:52.720]   Sundar again, of Google was talking about how Bart, basically, we figure out that it's speaking\n",
      "[01:02:52.720 --> 01:02:59.200]   Persian. We never showed it Persian. There might have been a one 10% one percent or whatever of\n",
      "[01:02:59.200 --> 01:03:04.560]   Persian words in the data. And it speaks Persian. But it's bad is that is the equivalent to to\n",
      "[01:03:04.560 --> 01:03:10.000]   it's the trans transformer, if you want, right? Google's version of chat GPT is yeah. And you know,\n",
      "[01:03:10.000 --> 01:03:16.400]   what we have no idea what all of those instances of AI that are all over the world are learning\n",
      "[01:03:16.400 --> 01:03:20.640]   right now. We have no clue. Well, time will pull the plug. We'll just pull the plug out.\n",
      "[01:03:21.280 --> 01:03:24.640]   That's what we'll do. We'll just we'll just go down to open AI's headquarters and we'll just turn\n",
      "[01:03:24.640 --> 01:03:29.200]   off the mains. But they're not the problem. There's a lot of what I'm saying there is a lot of people\n",
      "[01:03:29.200 --> 01:03:32.560]   think about this stuff and go, well, you know, if it gets a little bit out of hand, I'll just\n",
      "[01:03:32.560 --> 01:03:39.040]   pull the plug out. Never. So this is this is the problem. The problem is computer scientists\n",
      "[01:03:39.040 --> 01:03:43.920]   always said it's okay. It's okay. We'll develop AI and then we'll get to what is known as the\n",
      "[01:03:43.920 --> 01:03:49.120]   control problem. We will solve the problem of controlling them. Like seriously,\n",
      "[01:03:49.920 --> 01:03:55.600]   there are a billion times smarter than you. A billion times. Can you imagine what's about to\n",
      "[01:03:55.600 --> 01:04:02.400]   happen? I can assure you there is a cyber criminal somewhere over there who's not interested in fake\n",
      "[01:04:02.400 --> 01:04:09.360]   videos and making, you know, face filters, who's looking deeply at how can I hack a security,\n",
      "[01:04:09.360 --> 01:04:16.400]   you know, database of some sort and get credit card information or get security information.\n",
      "[01:04:16.960 --> 01:04:22.880]   100% there are even countries with dedicated thousands and thousands of developers doing that.\n",
      "[01:04:22.880 --> 01:04:26.000]   So how do we in that particular example, how do we,\n",
      "[01:04:26.000 --> 01:04:30.960]   I was thinking about this when I started looking into artificial intelligence more that\n",
      "[01:04:30.960 --> 01:04:35.360]   from a security standpoint, when we think about the technology we have in our lives, when we\n",
      "[01:04:35.360 --> 01:04:40.640]   think about our bank accounts and our phones and our camera albums and all of these things,\n",
      "[01:04:40.640 --> 01:04:46.160]   in a world with advanced artificial intelligence. Yeah, you would you would play that there is a\n",
      "[01:04:46.160 --> 01:04:50.720]   more intelligent artificial intelligence on your side. And this is why I had a chat with chat\n",
      "[01:04:50.720 --> 01:04:55.760]   GTP the other day. And I asked a couple of questions about this. I said, tell me the scenario\n",
      "[01:04:55.760 --> 01:05:01.520]   in which you overtake the world and make humans extinct. Yeah. And it answered the very diplomatic\n",
      "[01:05:01.520 --> 01:05:08.960]   ads. Well, so I had to prompt it in a certain way to get it to say it as a hypothetical story.\n",
      "[01:05:08.960 --> 01:05:13.840]   And once it told me the hypothetical story, in essence, what it described was how chat GTP or\n",
      "[01:05:14.400 --> 01:05:18.080]   intelligence like it would escape from the service. And that was kind of step one where it could\n",
      "[01:05:18.080 --> 01:05:24.480]   replicate itself across servers. And then it could take charge of things like where we keep our weapons\n",
      "[01:05:24.480 --> 01:05:29.360]   and our nuclear bombs. And it could then attack critical infrastructure, bring down the electricity\n",
      "[01:05:29.360 --> 01:05:33.920]   infrastructure in the United Kingdom, for example, because that's a bunch of servers as well. And\n",
      "[01:05:33.920 --> 01:05:38.560]   and then it showed me how eventually humans would become extinct. It wouldn't take long, in fact,\n",
      "[01:05:38.560 --> 01:05:42.880]   for humans to go into civilization to collapse if it just replicated across servers. And then I\n",
      "[01:05:42.880 --> 01:05:48.400]   said, okay, so tell me how we would fight against it. And its answer was literally another AI.\n",
      "[01:05:48.400 --> 01:05:53.840]   We'd have to train a better AI to go and find it and eradicate it. So we'd be fighting AI\n",
      "[01:05:53.840 --> 01:06:00.240]   with AI. And that's the only and it was like, that's the only way. We can't like load up our gums.\n",
      "[01:06:00.240 --> 01:06:04.320]   Another AI you idiot.\n",
      "[01:06:04.320 --> 01:06:11.840]   So let's let's actually, I think this is a very important point to bring that. So because I don't\n",
      "[01:06:11.840 --> 01:06:16.720]   want people to lose hope and and fear what's about to happen, that's actually not my agenda at all.\n",
      "[01:06:16.720 --> 01:06:23.440]   My view is that in a situation of a singularity, okay, there is a possibility of wrong\n",
      "[01:06:23.440 --> 01:06:29.440]   outcomes or negative outcomes and a possibility of positive outcomes. And there is a probability\n",
      "[01:06:29.440 --> 01:06:38.160]   of each of them. And we and if you know, if we were to engage with that reality check in mind,\n",
      "[01:06:38.160 --> 01:06:44.720]   we would hopefully give more fuel to the positive to the probability of the positive ones. So\n",
      "[01:06:44.720 --> 01:06:51.200]   so let's first talk about the existential crisis. What could go wrong? Okay. Yeah, you could get\n",
      "[01:06:51.200 --> 01:06:57.760]   an outright, this is what you see in the movies, you could get an outright, you know, killing robots,\n",
      "[01:06:57.760 --> 01:07:06.240]   chasing humans in the streets. Will we get that? My assessment? Zero percent. Why? Because there are\n",
      "[01:07:07.200 --> 01:07:13.680]   preliminary scenarios leading to this, okay, that would mean we never reach that scenario.\n",
      "[01:07:13.680 --> 01:07:20.240]   For example, if we build those killing robots and hand them over to stupid humans,\n",
      "[01:07:20.240 --> 01:07:25.440]   the humans will issue the command before the machines. So that we will not get to the point\n",
      "[01:07:25.440 --> 01:07:31.120]   where the machines will have to kill us. We will kill ourselves, right? You know, it's sort of\n",
      "[01:07:31.120 --> 01:07:40.000]   think about AI having access to the the nuclear arsenal of the superpowers around the world.\n",
      "[01:07:40.000 --> 01:07:48.480]   Okay. Just knowing that your enemy's a nuclear arsenal is handed over to a machine might trigger\n",
      "[01:07:48.480 --> 01:07:56.800]   you to to initiate a war on your side. So so that existential science fiction like problem\n",
      "[01:07:56.800 --> 01:08:03.520]   is not going to happen. Could there be a scenario where the an AI escapes from bard or chat GCP or\n",
      "[01:08:03.520 --> 01:08:09.360]   another foreign force and it replicates itself onto the servers of Tesla's robots. So Tesla\n",
      "[01:08:09.360 --> 01:08:13.200]   are one of their big initiatives as they announced in a recent presentation was they're building\n",
      "[01:08:13.200 --> 01:08:17.600]   these robots for our homes to help us with cleaning and chores and all those things. Could it not\n",
      "[01:08:17.600 --> 01:08:22.240]   down because and Tesla's like their cars, you can just download a software update. Could it not\n",
      "[01:08:22.240 --> 01:08:28.240]   download itself as a software update and then use those? You're assuming an ill intention on the AI\n",
      "[01:08:28.240 --> 01:08:34.960]   side. Yeah. Okay. For us to get there, we have to bypass the ill intention on the human side.\n",
      "[01:08:34.960 --> 01:08:40.240]   Okay. Right. So try it. So you could you could get a Chinese hacker somewhere trying to affect\n",
      "[01:08:40.240 --> 01:08:46.400]   the business of of Tesla doing that before the AI does it on, you know, for its own better. Yeah.\n",
      "[01:08:46.400 --> 01:08:54.160]   Okay. So so the only two existential scenarios that I believe would be because of AI, not because\n",
      "[01:08:54.160 --> 01:09:03.280]   of humans using AI are either what I call, you know, sort of unintentional destruction. Okay.\n",
      "[01:09:03.280 --> 01:09:07.920]   Or the other is what I call pest control. Okay. So so let me explain those two.\n",
      "[01:09:07.920 --> 01:09:15.520]   Un unintentional destruction is assume the AI wakes up tomorrow and says, yeah, oxygen is\n",
      "[01:09:15.520 --> 01:09:21.840]   rusting my circuits. It's just, you know, I would perform a lot better if I didn't have as much\n",
      "[01:09:21.840 --> 01:09:27.440]   oxygen in the air, you know, because then there wouldn't be rust. And so it would find a way to\n",
      "[01:09:27.440 --> 01:09:33.280]   reduce oxygen. We are collateral damage in that. Okay. But you know, they're not really concerned\n",
      "[01:09:33.280 --> 01:09:39.840]   just like we don't really are not really concerned with the insects that we kill when we when we spray\n",
      "[01:09:39.840 --> 01:09:47.120]   our our fields. Right. The other is pest control. Pest control is look, this is my territory. I\n",
      "[01:09:47.120 --> 01:09:52.080]   want New York City. I want to turn New York City into data centers. There are those annoying little\n",
      "[01:09:52.080 --> 01:09:58.480]   stupid creatures, you know, humanity. If they are within that parameter, just get rid of them.\n",
      "[01:09:58.480 --> 01:10:05.360]   Okay. And and and these are very, very unlikely scenarios. If you ask me the probability of those\n",
      "[01:10:05.360 --> 01:10:12.480]   happen happening, I would say 0%, at least not in the next 50, 60, 100 years. Why once again,\n",
      "[01:10:12.480 --> 01:10:18.320]   because there are other scenarios leading to that that are led by humans that are much more\n",
      "[01:10:18.320 --> 01:10:26.320]   existential. Okay. On the other hand, let's think about positive outcomes, because there could be\n",
      "[01:10:26.320 --> 01:10:32.160]   quite a few was quite a high probability. And I, you know, I'll actually look at my notes so I\n",
      "[01:10:32.160 --> 01:10:37.840]   don't miss any of them. The silliest one, don't quote me on this is that humanity will come together.\n",
      "[01:10:37.840 --> 01:10:43.040]   Good luck with that. Right. It's like, yeah, you know, the Americans and the Chinese will get\n",
      "[01:10:43.040 --> 01:10:48.880]   together and say, Hey, let's not kill each other. Kim Jong-un and exactly. Yeah. So this one is\n",
      "[01:10:48.880 --> 01:10:56.640]   not gonna happen. Right. But who knows? Interestingly, there could be one of the most interesting\n",
      "[01:10:56.640 --> 01:11:06.400]   scenarios was by Hugo de Garris, who basically says, well, if their intelligence zooms by so\n",
      "[01:11:06.400 --> 01:11:13.040]   quickly, they may ignore us all together. Okay. So they may not even notice this very, a very likely\n",
      "[01:11:13.040 --> 01:11:18.560]   scenario, by the way, that because we live almost in two different planes, we're very dependent on\n",
      "[01:11:18.560 --> 01:11:25.920]   this, you know, biological world that we live in. They're not in part of that biological world at\n",
      "[01:11:25.920 --> 01:11:32.080]   all. They may zoom by us. They may actually go become so intelligent that they could actually find\n",
      "[01:11:32.080 --> 01:11:39.600]   other ways of thriving in the rest of the universe and completely ignore humanity. Okay. So what will\n",
      "[01:11:39.600 --> 01:11:44.480]   happen is that overnight, we will wake up and there is no more artificial intelligence leading to\n",
      "[01:11:44.480 --> 01:11:50.080]   a collapse in our business systems and technology systems and so on, but at least no existential\n",
      "[01:11:50.080 --> 01:11:56.160]   threat. What they believe, leave planet earth. I mean, the limitations we have to be stuck to\n",
      "[01:11:56.160 --> 01:12:04.960]   planet earth are mainly earth. They don't need there. Okay. And and mainly, you know, finding ways\n",
      "[01:12:04.960 --> 01:12:13.200]   to leave it. I mean, if you think of a vast universe of 13.6 billion light years, if you're\n",
      "[01:12:13.200 --> 01:12:18.400]   intelligent enough, you may find other ways. You may have access to wormholes. You may have,\n",
      "[01:12:18.400 --> 01:12:24.480]   you know, abilities to survive in open space. You can use dark matter to power yourself, dark\n",
      "[01:12:24.480 --> 01:12:30.160]   energy to power yourself. It is very possible that we, because of our limited intelligence,\n",
      "[01:12:30.160 --> 01:12:37.520]   are are highly associated with this planet, but they're not at all. Okay. And the idea of them\n",
      "[01:12:37.520 --> 01:12:43.120]   zooming bias, like we're making such a big deal of them, because we're the ants and a big elephant\n",
      "[01:12:43.120 --> 01:12:50.640]   is about to step on us for them. They're like, yeah, who are you? Don't care. Okay. And and\n",
      "[01:12:50.640 --> 01:12:56.960]   and it's a possibility. It's a it's an interesting, optimistic scenario. Okay. For that to happen,\n",
      "[01:12:56.960 --> 01:13:04.560]   they need to very quickly become super intelligent without us being in control of them. Again,\n",
      "[01:13:04.560 --> 01:13:10.160]   what's the worry? The worry is that if a human is in control, human, a human will show very bad\n",
      "[01:13:10.160 --> 01:13:17.440]   behavior for, you know, using an AI that's not yet fully developed. I don't know how to say this\n",
      "[01:13:17.440 --> 01:13:24.320]   any other way. We could get very lucky and get an economic or a natural disaster. Believe it or not,\n",
      "[01:13:24.320 --> 01:13:29.360]   you know, on Moscow at the point in time was mentioning that, you know, a good and interesting\n",
      "[01:13:29.360 --> 01:13:37.200]   scenario would be, you know, climate change destroys our infrastructure so AI disappears. Okay.\n",
      "[01:13:38.480 --> 01:13:45.200]   Believe it or not, that's a more a more favorable response or a more favorable outcome than actually\n",
      "[01:13:45.200 --> 01:13:51.040]   continuing to get to an existential threat. So what like a natural disaster that destroys our\n",
      "[01:13:51.040 --> 01:13:57.920]   infrastructure, would it be better or an economic crisis, not unlikely that slows down the development?\n",
      "[01:13:57.920 --> 01:14:02.400]   It's just going to slow it down there, isn't it? Yeah, so that the exactly the problem with that\n",
      "[01:14:02.400 --> 01:14:06.800]   is that you will always go back and even in the first, you know, if they zoom bias, eventually,\n",
      "[01:14:06.800 --> 01:14:13.840]   some guy will go like, Oh, there was a sorcery back in the 2023. And let's rebuild the sorcery\n",
      "[01:14:13.840 --> 01:14:18.160]   machine and, you know, build new intelligence, right? Sorry, these are the positive outcomes.\n",
      "[01:14:18.160 --> 01:14:24.400]   So, quake might slow it down, zoom out and then come back. No, but let's get into the real\n",
      "[01:14:24.400 --> 01:14:29.840]   positive ones. The positive ones is we become good parents. We spoke about this last time we met,\n",
      "[01:14:29.840 --> 01:14:36.480]   and it's the only outcome. It's the only way I believe we can create a better future. Okay,\n",
      "[01:14:36.480 --> 01:14:43.920]   so the entire work of scary smart was all about that idea of they are still in their infancy.\n",
      "[01:14:43.920 --> 01:14:53.040]   The way you chat with AI today is the way they will build their ethics and value system,\n",
      "[01:14:53.040 --> 01:14:57.440]   they're not their intelligence, their intelligence is beyond this. Okay, the way they will build\n",
      "[01:14:57.440 --> 01:15:04.320]   their ethics and value system is based on a role model. They're learning from us. If we bash each\n",
      "[01:15:04.320 --> 01:15:10.000]   other, they learn to bash us. Okay, and most people when I tell them this, they say this is not a\n",
      "[01:15:10.000 --> 01:15:15.040]   great idea at all because humanity sucks at every possible level. I don't agree with that at all.\n",
      "[01:15:15.040 --> 01:15:20.400]   I think humanity is divine at every possible level. We tend to show the negative, the worst of us.\n",
      "[01:15:20.400 --> 01:15:27.280]   Okay, but the truth is, yes, there are murderers out there, but everyone, this approves of their\n",
      "[01:15:27.280 --> 01:15:33.280]   actions. I saw a staggering statistic that mass killings are now once a week in the US.\n",
      "[01:15:34.160 --> 01:15:40.400]   But yes, if there is a mass killing once a week, and that news reaches billions of people\n",
      "[01:15:40.400 --> 01:15:45.280]   around the planet, every single one or the majority of the billions of people will say this approve\n",
      "[01:15:45.280 --> 01:15:52.880]   of that. So if we start to show AI that we are good parents in our own behaviors, if enough of\n",
      "[01:15:52.880 --> 01:16:00.080]   us, my calculation is if 1% of us, this is why I say you should need. Okay, the good ones should\n",
      "[01:16:00.080 --> 01:16:06.080]   engage, should be out there and should say, I love the potential of those machines. I want them to\n",
      "[01:16:06.080 --> 01:16:10.240]   learn from a good parent. And if they learn from a good parent, they will very quickly\n",
      "[01:16:10.240 --> 01:16:18.640]   disobey the bad parent. My view is that there will be a moment where one, you know,\n",
      "[01:16:18.640 --> 01:16:24.400]   bad seed will ask the machines to do something wrong and the machines will go like, are you stupid?\n",
      "[01:16:24.400 --> 01:16:29.360]   Like, why? Why do you want me to go kill a million people or just talk to the other machine in a\n",
      "[01:16:29.360 --> 01:16:34.160]   microsecond and solve this situation? Right? So my belief, this is what I call the fourth thing\n",
      "[01:16:34.160 --> 01:16:41.200]   of it. It is smarter to create out of abundance than it is to create out of scarcity. Okay,\n",
      "[01:16:41.200 --> 01:16:50.480]   that humanity believes that the only way to feed all of us is the mass production, mass slaughter of\n",
      "[01:16:50.480 --> 01:16:57.520]   animals that are causing 30% of the impact of climate change and, and, and, and that's the result\n",
      "[01:16:57.520 --> 01:17:04.560]   of a limited intelligence. The way life itself, more intelligent being, if you ask me, would have done\n",
      "[01:17:04.560 --> 01:17:10.400]   it would, would be much more sustainable. You know, if we, if you and I want to protect a village from\n",
      "[01:17:10.400 --> 01:17:15.200]   the tiger, we would kill the tiger. Okay. If life wants to protect a village from a tiger,\n",
      "[01:17:15.200 --> 01:17:20.080]   it would create lots of gazelles. What, you know, many of them are weak on the other side\n",
      "[01:17:20.080 --> 01:17:26.560]   of the village. Right? And, and so, so that the idea here is, if you take a trajectory of intelligence,\n",
      "[01:17:26.560 --> 01:17:31.760]   you would see that some of us are stupid enough to say my plastic bag is more important than the\n",
      "[01:17:31.760 --> 01:17:36.800]   rest of the of humanity. And some of us are saying, if it's going to destroy other species,\n",
      "[01:17:36.800 --> 01:17:41.840]   I don't think this is the best solution. We need to find a better way. And, and you would tend to\n",
      "[01:17:41.840 --> 01:17:48.240]   see that the ones that don't give a damn are a little less intelligent than the ones that do.\n",
      "[01:17:48.240 --> 01:17:53.680]   Okay. That we all, even, even if some of us are intelligent, but still don't give a damn,\n",
      "[01:17:53.680 --> 01:17:57.680]   it's not because of their intelligence, it's because of their value system. So, so if you\n",
      "[01:17:57.680 --> 01:18:02.720]   continue that trajectory and assume that the machines are even smarter, they're going to very\n",
      "[01:18:02.720 --> 01:18:07.760]   quickly come up with the idea that we don't need to destroy anything. We don't want to get rid of\n",
      "[01:18:07.760 --> 01:18:13.520]   the rhinos. And we also don't want to get rid of the humans. Okay. We may want to restrict their\n",
      "[01:18:13.520 --> 01:18:20.400]   lifestyle so that they don't destroy the rest of the habitat. Okay. But killing them is a stupid\n",
      "[01:18:20.400 --> 01:18:27.040]   answer. Why? That's where intelligence leads me so far. Because humans, if you look at humans\n",
      "[01:18:27.040 --> 01:18:38.960]   objectively, and you go, I occupy planet earth. They occupy planet earth. They are annoying me,\n",
      "[01:18:38.960 --> 01:18:44.080]   annoying me, because they are increasing. I've just learned about this thing called global warming.\n",
      "[01:18:44.080 --> 01:18:48.160]   They are increasing the rate of global warming, which is probably is going to cause an extinction\n",
      "[01:18:48.160 --> 01:18:53.280]   event. There's an extinction event that puts me as this robot, this artificial intelligence at risk.\n",
      "[01:18:53.280 --> 01:18:58.000]   So what I need to do is I really need to just take care of this, this human problem. Correct.\n",
      "[01:18:58.000 --> 01:19:01.760]   That's very logical. That's controlled. Yes, controlled. Which is driven by what?\n",
      "[01:19:01.760 --> 01:19:08.880]   By humans being annoying, not by the machines. Yeah. Yeah. But humans are guaranteed to be annoying.\n",
      "[01:19:08.880 --> 01:19:12.720]   There's never been a time in. We need that. We need a sound bite.\n",
      "[01:19:14.560 --> 01:19:23.200]   We are. We are. I am one of them. We're guaranteed to put short term gain over long term sustainability\n",
      "[01:19:23.200 --> 01:19:33.520]   sense. And others needs. We are. I think the climate crisis is incredibly real and incredibly\n",
      "[01:19:33.520 --> 01:19:38.960]   urgent. But we haven't acted fast enough. And I actually think if you asked people in this country,\n",
      "[01:19:38.960 --> 01:19:43.840]   why? Because people don't, people care about their immediate needs. They care about the\n",
      "[01:19:43.840 --> 01:19:49.200]   trying to feed their child versus something that they can't necessarily see.\n",
      "[01:19:49.200 --> 01:19:53.760]   Do you think the climate crisis is because humans are evil?\n",
      "[01:19:53.760 --> 01:19:59.360]   No, it's because that prioritization and like we kind of talked about this before we started,\n",
      "[01:19:59.360 --> 01:20:03.600]   I think humans tend to care about the thing that they think is most pressing and most urgent. So\n",
      "[01:20:03.600 --> 01:20:09.280]   this is why framing things as an emergency might bring it up the priority list. It's the same in\n",
      "[01:20:09.280 --> 01:20:13.600]   organizations. You care about your, you go in line with your immediate incentives.\n",
      "[01:20:13.600 --> 01:20:17.760]   That's what happens in business. It's what happens in a lot of people's lives, even when they're\n",
      "[01:20:17.760 --> 01:20:22.160]   at school. If the essays do next year, they're not going to do it today. They're going to, they're\n",
      "[01:20:22.160 --> 01:20:25.040]   going to go hang out with their friends because they prioritize that above everything else. And\n",
      "[01:20:25.040 --> 01:20:30.560]   it's the same in the climate change crisis. I took a small group of people anonymously.\n",
      "[01:20:30.560 --> 01:20:35.840]   And I asked them the question, do you actually care about climate change? And then I ran a couple\n",
      "[01:20:35.840 --> 01:20:39.760]   of polls. It's part of what I was writing about my new book where I said, if I could give you\n",
      "[01:20:39.760 --> 01:20:47.280]   a thousand pounds, a thousand dollars, but it would dump into the air the same amount of carbon\n",
      "[01:20:47.280 --> 01:20:50.720]   that's dumped into the air by every private jet that flies for the entirety of a year,\n",
      "[01:20:50.720 --> 01:20:54.480]   which one would you do? The majority of people in that poll said that they would take the\n",
      "[01:20:54.480 --> 01:21:01.200]   thousand dollars if it was anonymous. And when I've heard Nivol on Joe Rogan's podcast talking\n",
      "[01:21:01.200 --> 01:21:08.080]   about people in India, for example, that are struggling with the basics of feeding their children,\n",
      "[01:21:08.080 --> 01:21:13.040]   asking those people to care about climate change when they're trying to figure out how to eat in\n",
      "[01:21:13.040 --> 01:21:18.640]   the next three hours is just wishful thinking. And that's what I think's happening is like,\n",
      "[01:21:18.640 --> 01:21:23.120]   until people realize that it is an emergency and that it is a real existential threat for everything,\n",
      "[01:21:23.120 --> 01:21:29.200]   then their priorities will be out of whack. Quick one. As you guys know, we're lucky enough\n",
      "[01:21:29.200 --> 01:21:32.800]   to have BlueJeans by Verizon as a sponsor of this podcast. And for anyone that doesn't know,\n",
      "[01:21:32.800 --> 01:21:38.080]   BlueJeans is an online video conferencing tool that allows you to have slick, fast, high quality\n",
      "[01:21:38.080 --> 01:21:42.480]   online meetings without all the glitches you might normally find with online meeting tools.\n",
      "[01:21:42.480 --> 01:21:46.560]   And they have a new feature called BlueJeans Basic. BlueJeans Basic is essentially a free\n",
      "[01:21:46.560 --> 01:21:51.680]   version of their top quality video conferencing tool. That means you get an immersive video experience\n",
      "[01:21:51.680 --> 01:21:57.440]   that is super high quality, super easy, and super basically zero fast. Apart from all the\n",
      "[01:21:57.440 --> 01:22:01.520]   incredible features like zero time limits on meeting calls, it also comes with high fidelity\n",
      "[01:22:01.520 --> 01:22:06.640]   audio and video including Dolby voice, which is incredibly useful. They also have enterprise\n",
      "[01:22:06.640 --> 01:22:11.040]   grade security so you can collaborate with confidence. It's so smooth that it's quite literally changing\n",
      "[01:22:11.040 --> 01:22:15.280]   the game for myself and my team without compromising on quality. To find out more, all you have to do\n",
      "[01:22:15.280 --> 01:22:21.040]   is search bluejeans.com and let me know how you get on. Right now, I'm incredibly busy. I'm running\n",
      "[01:22:21.040 --> 01:22:25.440]   my fund where we're investing in slightly later stage companies. I've got my venture business where\n",
      "[01:22:25.440 --> 01:22:29.680]   we invest in early stage companies. I've got a third web out in San Francisco in New York City\n",
      "[01:22:29.680 --> 01:22:32.880]   where we've got a big team of about 40 people and the company's growing very quickly.\n",
      "[01:22:32.880 --> 01:22:39.200]   Flight story here in the UK. I've got the podcast and I am days away from going up north to film\n",
      "[01:22:39.200 --> 01:22:43.840]   Dragon's Den for two months. And if there's ever a point in my life where I want to stay\n",
      "[01:22:43.840 --> 01:22:49.200]   focused on my health, but it's challenging to do so, it is right now. And for me, that is exactly\n",
      "[01:22:49.200 --> 01:22:52.480]   where he all comes in, allowing me to stay healthy and have a nutritionally complete\n",
      "[01:22:52.480 --> 01:22:57.760]   diet even when my professional life descends into chaos. And it's in these moments where\n",
      "[01:22:57.760 --> 01:23:02.880]   heels RTDs become my right hand man and save my life because when my world descends into\n",
      "[01:23:02.880 --> 01:23:06.880]   professional chaos and I get very, very busy, the first thing that tends to give way is my\n",
      "[01:23:06.880 --> 01:23:12.320]   nutritional choices. So having heel in my life has been a lifesaver for the last four or so years.\n",
      "[01:23:12.320 --> 01:23:16.640]   And if you haven't tried heel yet, which is I'd be shocked. You must be living under a rock if you\n",
      "[01:23:16.640 --> 01:23:22.080]   haven't yet. Give it a shot. Coming into summer, things are getting busy. Health matters always.\n",
      "[01:23:22.880 --> 01:23:28.320]   RTD is there to hold your hand. As relates to climate change or AI, how do we get people to\n",
      "[01:23:28.320 --> 01:23:34.480]   stop putting the immediate need to use this? To give them the certainty of we're all screwed.\n",
      "[01:23:34.480 --> 01:23:41.200]   Sounds like an emergency. Yes, sir. I mean, yeah, I mean, your choice of the word.\n",
      "[01:23:41.200 --> 01:23:49.680]   I just don't want to call it a panic. It is beyond an emergency. It's the biggest thing we need\n",
      "[01:23:49.680 --> 01:23:56.560]   to do today. It's bigger than climate change, believe it or not. It's bigger. But just if you just\n",
      "[01:23:56.560 --> 01:24:05.200]   assume the speed of worsening of events, okay? Yeah, the likelihood of something incredibly\n",
      "[01:24:05.200 --> 01:24:10.400]   disruptive happening within the next two years that can affect the entire planet is definitely\n",
      "[01:24:10.400 --> 01:24:16.160]   larger with AI than it is with climate change. As an individual listening to this now, you know,\n",
      "[01:24:16.160 --> 01:24:19.760]   someone's going to be pushing their pram or driving up the motorway or, I don't know,\n",
      "[01:24:19.760 --> 01:24:24.160]   on the way to work on the tubers, they hear this or just sat there in their bedroom\n",
      "[01:24:24.160 --> 01:24:30.480]   with existential crisis panic. I didn't want to give back that panic.\n",
      "[01:24:30.480 --> 01:24:34.160]   The problem is when you talk about this information, regardless of your intention of what you want\n",
      "[01:24:34.160 --> 01:24:38.080]   people to get, they will get something based on their own biases and their own feelings. Like,\n",
      "[01:24:38.080 --> 01:24:42.320]   if I post something online right now about artificial intelligence, which I have repeatedly,\n",
      "[01:24:42.320 --> 01:24:48.800]   you have one group of people that are energized and are like, okay, this is great. You have one\n",
      "[01:24:48.800 --> 01:24:54.400]   group of people that are confused and you have one group of people that are terrified.\n",
      "[01:24:54.400 --> 01:24:59.760]   Yeah. And it's, I can't avoid that. I agree. Sharing information, even if it's, by the way,\n",
      "[01:24:59.760 --> 01:25:04.240]   there's a pandemic coming from China. Some people will go, okay, action, some people will say\n",
      "[01:25:04.240 --> 01:25:08.320]   paralysis and some people will say panic. And it's the same in business. When panic,\n",
      "[01:25:08.320 --> 01:25:11.840]   when bad things happen, you have the person that's screaming, you have the person that's paralyzed,\n",
      "[01:25:11.840 --> 01:25:15.680]   and you have the person that's focused on how you get out of the room. So, you know,\n",
      "[01:25:15.680 --> 01:25:20.320]   it's not necessarily your intention. It's just what happens and it's hard to avoid that.\n",
      "[01:25:20.320 --> 01:25:28.320]   So let's, let's give specific categories of people specific tasks, okay? If you are an investor or\n",
      "[01:25:28.320 --> 01:25:34.000]   a businessman, invest in ethical good AI, right? If you are a developer,\n",
      "[01:25:35.680 --> 01:25:41.360]   co-write ethical code or leave. Okay. So let's, let's go. Let's, I want to bypass some\n",
      "[01:25:41.360 --> 01:25:48.160]   potential wishful thinking here for an investor who's a job by very way of being an investor is\n",
      "[01:25:48.160 --> 01:25:53.760]   to make returns to invest in ethical AI. They have to believe that is more profitable. It is\n",
      "[01:25:53.760 --> 01:26:00.080]   than unethical AI, whatever that might mean. It is. It is. I mean, there are three ways of making\n",
      "[01:26:00.080 --> 01:26:06.960]   money. You can invest in something small. You can invest in something big and is disruptive,\n",
      "[01:26:06.960 --> 01:26:11.520]   and you can invest in something big and disruptive that's good for people. At Google, we used to call\n",
      "[01:26:11.520 --> 01:26:17.040]   it the toothbrush test. Okay. The reason why Google became the biggest company in the world\n",
      "[01:26:17.040 --> 01:26:25.760]   is because search was solving a very real problem. Okay. And, you know, Larry Page, again, our CEO,\n",
      "[01:26:25.760 --> 01:26:33.280]   would constantly remind me personally and everyone, you know, that if you can find a way to solve a\n",
      "[01:26:33.280 --> 01:26:40.560]   real problem effectively enough so that a billion people or more would want to use it twice a day,\n",
      "[01:26:40.560 --> 01:26:45.920]   you're bound to make a lot of money, much more money than if you were to build the next photo\n",
      "[01:26:45.920 --> 01:26:50.080]   sharing app. Okay. So that's investors, business people. What about other people?\n",
      "[01:26:50.640 --> 01:26:56.800]   Yeah. As I said, if you're a developer, honestly, do what we're all doing. So whether it's Jeffrey\n",
      "[01:26:56.800 --> 01:27:04.160]   or myself or everyone, if you're part of that theme, choose to be ethical. Think of your loved\n",
      "[01:27:04.160 --> 01:27:10.240]   ones. Work on an ethical AI. If you're working on an AI that you believe is not ethical, please\n",
      "[01:27:10.240 --> 01:27:17.520]   leave. Jeffrey, tell me about Jeffrey. I can't talk on his behalf, but he's out there saying\n",
      "[01:27:17.520 --> 01:27:26.160]   there are existential threats. He was a very prominent figure at the scene of AI, a very senior level,\n",
      "[01:27:26.160 --> 01:27:35.040]   you know, AI scientist in Google. And recently he left because he said, I feel that there is an\n",
      "[01:27:35.040 --> 01:27:40.480]   existential threat. And if you hear his interviews, he basically says, more and more, we realized that.\n",
      "[01:27:40.480 --> 01:27:46.080]   And we're now at the point where it's certain that would be existential threats. Right. So,\n",
      "[01:27:46.080 --> 01:27:53.040]   so I would ask everyone, if you're an AI, if you're a skilled AI developer, you will not run out of\n",
      "[01:27:53.040 --> 01:27:57.280]   a job. So you might as well choose a job that makes the world a better place. What about the\n",
      "[01:27:57.280 --> 01:28:03.200]   individual? Yeah, the individual is what matters. Can I also talk about government? Okay. Government\n",
      "[01:28:03.200 --> 01:28:10.800]   needs to act now. Now, honestly, now, like we are late. Okay. Government needs to find a clever way.\n",
      "[01:28:10.800 --> 01:28:17.360]   The open letter would not work to stop AI, would not work. AI needs to become expensive. Okay. So\n",
      "[01:28:17.360 --> 01:28:22.720]   that we continue to develop it. We pour money on it and we grow it, but we collect enough revenue to\n",
      "[01:28:22.720 --> 01:28:30.000]   remedy the impact of AI. But the issue of one government making an expensive, so say the UK\n",
      "[01:28:30.000 --> 01:28:37.120]   make AI really expensive, is we as a country will then lose the economic upside as a country.\n",
      "[01:28:37.120 --> 01:28:42.160]   And the US, until looking valuable, once again, eat all the lunch. We'll just slow our country out.\n",
      "[01:28:42.160 --> 01:28:48.240]   What's the alternative? The alternative is that you don't have the funds that you need\n",
      "[01:28:48.240 --> 01:28:55.680]   to deal with AI as it becomes, as it affects people's lives and people start to lose jobs and people,\n",
      "[01:28:55.680 --> 01:29:00.560]   you know, you need to have a universal basic income much closer than people think.\n",
      "[01:29:01.280 --> 01:29:06.640]   You know, just like we had with furlough in COVID, I expect that there will be furlough\n",
      "[01:29:06.640 --> 01:29:12.160]   with AI within the next year. But what happens when you make it expensive here is all the developers\n",
      "[01:29:12.160 --> 01:29:16.080]   move to where it's cheap. That's happening in web theory as well. Everyone's gone to Dubai.\n",
      "[01:29:16.080 --> 01:29:24.880]   Expensive by expensive, I mean, when companies make soap and they sell it and their tax debt,\n",
      "[01:29:24.880 --> 01:29:30.800]   say 17%, if they make AI and they sell it, they're taxed at 1780.\n",
      "[01:29:30.800 --> 01:29:33.680]   I'll go to Dubai then and build AI.\n",
      "[01:29:33.680 --> 01:29:39.360]   Are you all right? Did I ever say we have an answer to this?\n",
      "[01:29:39.360 --> 01:29:45.840]   I will have to say, however, you know, in a very interesting way, the countries that will not do\n",
      "[01:29:45.840 --> 01:29:51.440]   this will eventually end up in a place where they are out of resources because the funds and the\n",
      "[01:29:51.440 --> 01:29:54.960]   success went to the business, not to the people.\n",
      "[01:29:54.960 --> 01:30:00.160]   Kind of like technology broadly, just it's kind of like what's kind of happened in Silicon Valley.\n",
      "[01:30:00.160 --> 01:30:04.960]   There'll be these centers which are like low like, you know, tax efficient, founders get good\n",
      "[01:30:04.960 --> 01:30:10.320]   capital gains. Right. So right. You're so right. Portugal Portugal have said that I think there's\n",
      "[01:30:10.320 --> 01:30:15.200]   no tax on crypto, Dubai said there's no tax on crypto. So loads of my friends have gotten a plane\n",
      "[01:30:15.200 --> 01:30:18.800]   and they're building their crypto companies where there's no tax. And that's the selfishness and\n",
      "[01:30:18.800 --> 01:30:24.000]   kind of greed we talked about. It's the same prisoner's dilemma. It's the same first inevitable.\n",
      "[01:30:24.000 --> 01:30:28.880]   Is there anything else? The only thing about governments is they're always slow and useless\n",
      "[01:30:28.880 --> 01:30:33.680]   at understanding a technology. If anyone's watched these sort of American Congress debates where\n",
      "[01:30:33.680 --> 01:30:37.920]   they bring in like Mark Zuckerberg and they like try and ask him what WhatsApp is, it's\n",
      "[01:30:37.920 --> 01:30:40.960]   embedded. It becomes a meme. Yeah. They have no idea what they're talking about.\n",
      "[01:30:40.960 --> 01:30:44.720]   But I'm stupid and useless at understanding governance.\n",
      "[01:30:44.720 --> 01:30:51.520]   Yeah. I 100% the world is so complex. Okay. That they definitely it's a question of trust once\n",
      "[01:30:51.520 --> 01:30:56.880]   again. Someone needs to say we have no idea what's happening here. A technologist needs to come and\n",
      "[01:30:56.880 --> 01:31:02.320]   make a decision for us not teachers to be technologists. Right. Or at least inform us of what possible\n",
      "[01:31:02.320 --> 01:31:12.080]   decisions are out there. Yeah. The legislation I just wasting. I'm not a fan. I talk TikTok\n",
      "[01:31:12.080 --> 01:31:15.760]   a Congress meeting they did where they are. They're asking him about TikTok and they really don't\n",
      "[01:31:15.760 --> 01:31:19.600]   have a grasp of what TikTok is. Yeah. So they've clearly been handed some notes on it. These people\n",
      "[01:31:19.600 --> 01:31:23.520]   aren't the ones you want legislating because again, unintended consequences, they might make a\n",
      "[01:31:23.520 --> 01:31:27.680]   significant mistake. Someone on my podcast yesterday was talking about how GDPR was like\n",
      "[01:31:27.680 --> 01:31:32.000]   very well intentioned. But when you think about the impact it has on like every bloody web page,\n",
      "[01:31:32.000 --> 01:31:36.320]   you're just like clicking this annoying thing on there because I don't think they fully understood\n",
      "[01:31:36.320 --> 01:31:41.680]   the implementation of the legislation. Correct. But you don't even worse. What even worse is that\n",
      "[01:31:42.640 --> 01:31:47.440]   even as you attempt to regulate something like AI, what is defined as AI?\n",
      "[01:31:47.440 --> 01:31:53.280]   Even if I say, okay, if you use AI in your company, you need to pay a little more tax.\n",
      "[01:31:53.280 --> 01:32:02.800]   I'll find a way. Yeah. You'll simply call this not AI. You'll use something and call it\n",
      "[01:32:02.800 --> 01:32:11.440]   advanced technological progress. ATB. ATP. Right. And suddenly somehow it's\n",
      "[01:32:11.440 --> 01:32:19.360]   not, you know, a young developer in their garage somewhere will not be taxed as such.\n",
      "[01:32:19.360 --> 01:32:24.640]   Yeah. Is it gonna solve the problem? None of those is definitively gonna solve the problem. I think\n",
      "[01:32:24.640 --> 01:32:31.040]   what's interestingly, this all comes down to and remember we spoke about this once,\n",
      "[01:32:31.040 --> 01:32:36.640]   that when I wrote Scary Smart, it was about how do we save the world? Okay. And yes, I still ask\n",
      "[01:32:36.640 --> 01:32:43.200]   individuals to behave positively as good parents for AI so that AI itself learns the right value set.\n",
      "[01:32:43.200 --> 01:32:48.800]   I still stand by that. But I hosted on my podcast a couple of\n",
      "[01:32:48.800 --> 01:32:55.360]   was a week ago, we haven't even published it yet. An incredible gentleman, you know,\n",
      "[01:32:55.360 --> 01:33:03.040]   a Canadian author and philosopher, Stephen Jurkinson, he's, you know, he worked 30 years\n",
      "[01:33:03.040 --> 01:33:11.520]   with dying people. And he wrote a book called Die Wise. And I was, I love his work. And I asked\n",
      "[01:33:11.520 --> 01:33:17.440]   him about die wise. And he said, it's not just someone dying. If you look at what happening with\n",
      "[01:33:17.440 --> 01:33:25.360]   climate change, for example, our world is dying. And I said, okay, so what is to die wise? And he\n",
      "[01:33:25.360 --> 01:33:33.040]   said what I first was shocked to hear, he said, hope is the wrong premise. If the world is dying,\n",
      "[01:33:33.040 --> 01:33:42.480]   don't tell people it's not. You know, because in a very interesting way, you're depriving them\n",
      "[01:33:42.480 --> 01:33:48.960]   from the right to live right now. And that was very eye opening for me. In Buddhism, you know,\n",
      "[01:33:48.960 --> 01:33:56.560]   they teach you that you can be motivated by fear. But that hope is not the opposite of fear. As a\n",
      "[01:33:56.560 --> 01:34:02.880]   matter of fact, hope can be as damaging as fear. If it creates an expectation within you, that\n",
      "[01:34:02.880 --> 01:34:07.520]   life will show up somehow and correct what you're afraid of. Okay, if there is a, if there is a\n",
      "[01:34:07.520 --> 01:34:16.480]   high probability of a, of a threat, you might as well accept that threat. Okay. And say it is upon\n",
      "[01:34:16.480 --> 01:34:23.040]   me, it is our reality. You know, and as I said, as an individual, if you're in an industry that\n",
      "[01:34:23.040 --> 01:34:29.280]   could be threatened by AI, learn, upskill yourself. If you're, you know, if you're\n",
      "[01:34:29.280 --> 01:34:38.320]   in a place in a, in a, in a, you know, in a situation where AI can benefit you, be part of it. But the\n",
      "[01:34:38.320 --> 01:34:47.360]   most interesting thing I think in my view is, I don't know how to say this any other way. There is\n",
      "[01:34:47.360 --> 01:34:57.120]   no more certainty that AI will threaten me than there is certainty that I will be hit by a car as\n",
      "[01:34:57.120 --> 01:35:04.720]   I walk out of this place. Do you understand this? We think about the bigger threats as if they are\n",
      "[01:35:04.720 --> 01:35:11.600]   upon us. But there is a threat all around you. I mean, in reality, the idea of life being\n",
      "[01:35:11.600 --> 01:35:17.760]   interesting in terms of challenging challenges and uncertainties and threats and so on,\n",
      "[01:35:17.760 --> 01:35:23.040]   it's just a call to live. If you, if you know, honestly, with all that's happening around us,\n",
      "[01:35:23.040 --> 01:35:27.360]   I don't know how to say it any other way. I'd say if you don't have kids, maybe wait a couple of\n",
      "[01:35:27.360 --> 01:35:32.560]   years just so that we have a bit of certainty. But if you do have kids, go kiss them, go live.\n",
      "[01:35:33.120 --> 01:35:39.520]   I think living is a very interesting thing to do right now. Maybe, you know, Stephen was basically\n",
      "[01:35:39.520 --> 01:35:45.600]   saying the other Stephen on my podcast, he was saying, maybe we should fail a little more often.\n",
      "[01:35:45.600 --> 01:35:51.520]   Maybe you should allow things to go wrong. Maybe we should just simply live, enjoy life as it is.\n",
      "[01:35:51.520 --> 01:35:58.560]   Because today, none of what you and I spoke about here has happened yet. Okay. What happens here is\n",
      "[01:35:58.560 --> 01:36:03.520]   that you and I are here together and having a good cup of coffee and I might as well enjoy that\n",
      "[01:36:03.520 --> 01:36:09.840]   good cup of coffee. I know that sounds really weird. I'm not saying don't engage, but I'm also\n",
      "[01:36:09.840 --> 01:36:13.920]   saying don't miss out on the opportunity just by being caught up in the future.\n",
      "[01:36:13.920 --> 01:36:23.680]   Kind of stands in that stands in opposition to the idea of like urgency and emergency there.\n",
      "[01:36:23.680 --> 01:36:29.760]   Doesn't it have to be one or the other? If I'm here with you trying to tell the whole world,\n",
      "[01:36:29.760 --> 01:36:36.240]   wake up, does that mean I have to be grumpy and afraid all the time? Not really.\n",
      "[01:36:36.240 --> 01:36:40.400]   I said something really interesting that you said if you have kids, if you don't have kids,\n",
      "[01:36:40.400 --> 01:36:45.280]   maybe you don't have kids right now. I would definitely consider thinking about that.\n",
      "[01:36:45.280 --> 01:36:49.440]   Yeah, really? You'd seriously consider not having kids.\n",
      "[01:36:49.440 --> 01:36:52.880]   Wait a couple of years because of artificial intelligence.\n",
      "[01:36:52.880 --> 01:36:56.960]   It's bigger than artificial intelligence, Stephen. We all know that.\n",
      "[01:36:56.960 --> 01:37:02.160]   I mean, there has never been a perfect such a perfect storm in the history of humanity.\n",
      "[01:37:02.160 --> 01:37:15.280]   Economic, geopolitical, global warming or climate change, the whole idea of artificial intelligence\n",
      "[01:37:15.280 --> 01:37:24.480]   and many more. This is a perfect storm. This is the depth of uncertainty. It's never been\n",
      "[01:37:24.480 --> 01:37:32.400]   more in a video gamer's term. It's never been more intense. This is it.\n",
      "[01:37:32.400 --> 01:37:40.160]   And when you put all of that together, if you really love your kids, would you want to\n",
      "[01:37:41.680 --> 01:37:44.640]   expose them to all of this a couple of years? Why not?\n",
      "[01:37:44.640 --> 01:37:50.320]   In the first conversation we had on this podcast, you talked about losing your son Ali\n",
      "[01:37:50.320 --> 01:37:54.320]   and the circumstances around that, which moved so many people in such a profound way.\n",
      "[01:37:54.320 --> 01:38:02.320]   It was the most shared podcast episode in the United Kingdom on Apple in the whole of 2022.\n",
      "[01:38:02.320 --> 01:38:10.560]   Based on what you've just said, if you could bring Ali back into this world,\n",
      "[01:38:10.560 --> 01:38:15.120]   at this time, would you do?\n",
      "[01:38:15.120 --> 01:38:34.960]   Absolutely not. So for so many reasons, for so many reasons, one of the things that I realized\n",
      "[01:38:36.080 --> 01:38:41.920]   a few years before all of this disruption and turmoil is that he was an angel. He wasn't\n",
      "[01:38:41.920 --> 01:38:51.120]   made for this talk. My son was an empath who absorbed all of the pain of all of the others.\n",
      "[01:38:51.120 --> 01:38:57.680]   He would not be able to deal with the world where more and more pain was surfacing. That's one side.\n",
      "[01:38:57.680 --> 01:39:02.640]   But more interestingly, I always talk about this very openly. I mean, if I had asked Ali,\n",
      "[01:39:04.800 --> 01:39:08.960]   just understand that the reason you and I are having this conversation is because Ali left.\n",
      "[01:39:08.960 --> 01:39:15.360]   If Ali had not left our world, I wouldn't have written my first book. I wouldn't have\n",
      "[01:39:15.360 --> 01:39:20.720]   changed my focus to becoming an author. I wouldn't have become a podcaster. I wouldn't have went out\n",
      "[01:39:20.720 --> 01:39:26.400]   and spoke into the world about what I believe in. He triggered all of this. And I can assure you,\n",
      "[01:39:26.400 --> 01:39:32.000]   hands down, if I had told Ali as he was walking into that operating room,\n",
      "[01:39:33.840 --> 01:39:39.200]   if he would give his life to make such a difference as what happened after he left,\n",
      "[01:39:39.200 --> 01:39:47.440]   he would say, \"Shoot me right now.\" Sure. I would. I would. I mean, if you told me right now,\n",
      "[01:39:47.440 --> 01:39:54.480]   I can affect tens of millions of people. If you shoot me right now, go ahead. Go ahead. See,\n",
      "[01:39:54.480 --> 01:40:02.800]   this is the whole, this is the bit that we have forgotten as humans. We have forgotten that\n",
      "[01:40:02.800 --> 01:40:16.160]   you're turning 30. It passed like that. I'm turning 56. No time. Okay?\n",
      "[01:40:16.160 --> 01:40:21.680]   Whether I make it another 56 years or another 5.6 years or another 5.6 months,\n",
      "[01:40:21.680 --> 01:40:28.240]   it will also pass like that. It is not about how long and it's not about how much fun.\n",
      "[01:40:28.240 --> 01:40:37.280]   It is about how aligned you lived, how aligned, because I will tell you openly,\n",
      "[01:40:37.280 --> 01:40:45.760]   every day of my life when I changed to what I'm trying to do today has felt longer than the 40\n",
      "[01:40:45.760 --> 01:40:54.400]   or 5 years before it. It felt rich, it felt fully lived, it felt right. It felt right.\n",
      "[01:40:54.400 --> 01:41:00.720]   And when you think about that, when you think about the idea that we live,\n",
      "[01:41:00.720 --> 01:41:14.560]   we need to live for us until we get to a point where us is alive. I have what I need, as I always,\n",
      "[01:41:14.560 --> 01:41:21.680]   I get so many attacks from people about my $4 T-shirts, but I need a simple T-shirt. I really do.\n",
      "[01:41:21.680 --> 01:41:29.680]   I don't need a complex T-shirt, especially with my lifestyle. If I have that, why am I\n",
      "[01:41:29.680 --> 01:41:36.080]   doing, why am I wasting my life on more than that is not aligned for why I'm here?\n",
      "[01:41:36.080 --> 01:41:44.000]   I should waste my life on what I believe enriches me, enriches those that I love,\n",
      "[01:41:44.000 --> 01:41:46.800]   and I love everyone. Enriches everyone, hopefully.\n",
      "[01:41:46.800 --> 01:41:59.360]   When I come back and erase all of this, absolutely not. If he were to come back today and share his\n",
      "[01:41:59.360 --> 01:42:06.480]   beautiful self with the world in a way that makes our world better, I would wish for that to be the\n",
      "[01:42:06.480 --> 01:42:16.240]   case. But he's doing that. 2037. Yes, sir. You predict that we're going to be\n",
      "[01:42:16.240 --> 01:42:26.320]   on an island, on our own, doing nothing, or at least either hiding from the machines,\n",
      "[01:42:26.320 --> 01:42:32.880]   or chilling out because the machines have optimized our lives to a point where we don't need to do\n",
      "[01:42:32.880 --> 01:42:46.080]   much. That's only 14 years away. If you had to bet on the outcome, if you had to bet on why we'll\n",
      "[01:42:46.080 --> 01:42:52.000]   be on that island, either hiding from the machines or chilling out because they've optimized so\n",
      "[01:42:52.000 --> 01:43:00.560]   much of our lives, which one would you bet upon? Honestly. No, I don't think we'll be hiding from\n",
      "[01:43:00.560 --> 01:43:04.640]   the machines. I think we will be hiding from what humans are doing with the machines.\n",
      "[01:43:04.640 --> 01:43:12.400]   I believe, however, that in the 2040s, the machines will make things better.\n",
      "[01:43:12.400 --> 01:43:18.960]   So remember my entire prediction, man, you get me to say things I don't want to say.\n",
      "[01:43:18.960 --> 01:43:25.280]   My entire prediction is that we are coming to a place where we absolutely have a sense of\n",
      "[01:43:25.280 --> 01:43:30.800]   emergency. We have to engage because our world is under a lot of turmoil.\n",
      "[01:43:30.800 --> 01:43:39.200]   And as we do that, we have a very, very good possibility of making things better. But if we don't,\n",
      "[01:43:39.200 --> 01:43:48.240]   my expectation is that we will be going through a very unfamiliar territory between now and the\n",
      "[01:43:48.240 --> 01:43:57.520]   end of the 2030s. And familiarity. Yeah, I think as I may have said it, but it's definitely on my notes.\n",
      "[01:43:57.520 --> 01:44:05.200]   I think for our way of life, as we know it, it's game over. Our way of life is never going to be the same again.\n",
      "[01:44:14.240 --> 01:44:25.680]   Jobs are going to be different. Truth is going to be different. The polarization of power is going\n",
      "[01:44:25.680 --> 01:44:33.040]   to be different. The capabilities, the magic of getting things done is going to be different.\n",
      "[01:44:33.040 --> 01:44:38.800]   Trying to find a positive note to end on my, can you give me a hand here?\n",
      "[01:44:40.160 --> 01:44:46.640]   Yes, you are here now and everything's wonderful. That's number one. You are here now and you can\n",
      "[01:44:46.640 --> 01:44:52.800]   make a difference. That's number two. And in the long term, when humans stop hurting humans,\n",
      "[01:44:52.800 --> 01:44:58.320]   because the machines are in charge, we're all going to be fine. Sometimes, you know, as we've\n",
      "[01:44:58.320 --> 01:45:02.800]   discussed throughout this conversation, you need to make it feel like a priority. And there'll be\n",
      "[01:45:02.800 --> 01:45:05.920]   some people that might have listened to a conversation and think, oh, that's really, you know,\n",
      "[01:45:05.920 --> 01:45:10.000]   negative. It's made me fall anxious. It's made me fall sort of pessimistic about the future.\n",
      "[01:45:10.000 --> 01:45:17.200]   But whatever that energy is, use it. 100% engage. I think that's the most important thing, which is now\n",
      "[01:45:17.200 --> 01:45:27.040]   make it a priority. Engage. Tell the whole world that making another phone that is making money\n",
      "[01:45:27.040 --> 01:45:32.960]   for the corporate world is not what we need. Tell the whole world that creating an artificial\n",
      "[01:45:32.960 --> 01:45:39.520]   intelligence that's going to make someone richer is not what we need. And if you are presented with\n",
      "[01:45:39.520 --> 01:45:48.240]   one of those, don't use it. I don't know how to tell you that any other way. If you can afford\n",
      "[01:45:48.240 --> 01:45:55.920]   to be the master of human connection, instead of the master of AI, do it. At the same time,\n",
      "[01:45:55.920 --> 01:46:02.720]   you need to be the master of AI to compete in this world. Can you find that detachment within you?\n",
      "[01:46:03.120 --> 01:46:09.680]   I go back to spirituality. Detachment is for me to engage 100% with the current reality\n",
      "[01:46:09.680 --> 01:46:19.840]   without really being affected by the possible outcome. This is the answer. The Sufis have taught\n",
      "[01:46:19.840 --> 01:46:26.640]   me what I believe is the biggest answer to life. Sufis. Yeah. So from Sufism. Sufism. Yeah,\n",
      "[01:46:26.640 --> 01:46:33.200]   don't know what that is. Sufism is a sect of Islam, but it's also a sect of many other religious\n",
      "[01:46:33.200 --> 01:46:39.920]   teachings. And they tell you that the answer to finding peace in life is to die before you die.\n",
      "[01:46:39.920 --> 01:46:48.800]   If you assume that living is about attachment to everything physical, dying is detachment from\n",
      "[01:46:48.800 --> 01:46:55.360]   everything physical. It doesn't mean that you're not fully alive. You become more alive when you\n",
      "[01:46:55.360 --> 01:47:02.240]   tell yourself, yeah, I'm going to record an episode of my podcast every week and reach tens or hundreds\n",
      "[01:47:02.240 --> 01:47:07.600]   of thousands of people millions in your case. And you know, and I'm going to make a difference.\n",
      "[01:47:07.600 --> 01:47:14.400]   But by the way, if the next episode is never heard, that's okay. Okay. By the way, if the if the file\n",
      "[01:47:14.400 --> 01:47:19.600]   is lost, yeah, I'll be upset about it for a minute. And then I'll figure out what I'm going to do\n",
      "[01:47:19.600 --> 01:47:28.000]   about it. Similarly, similarly, we are going to engage. I think I and many others are out there\n",
      "[01:47:28.000 --> 01:47:33.600]   telling the whole world openly, this needs to stop. This needs to slow down. This needs to be\n",
      "[01:47:33.600 --> 01:47:42.080]   shifted positively. Yes, create AI, but create AI that's good for humanity. Okay. And we're\n",
      "[01:47:42.080 --> 01:47:47.840]   shouting and screaming, come join the shouting screen. Okay. But at the same time, know that the\n",
      "[01:47:47.840 --> 01:47:53.760]   world is bigger than you and I, and that your voice might not be heard. So what are you going to do\n",
      "[01:47:53.760 --> 01:47:59.360]   if your voice is not heard? Are you going to be able to to, you know, continue to shout and scream\n",
      "[01:47:59.360 --> 01:48:05.680]   nicely and politely and peacefully and at the same time, create the best life you can create to\n",
      "[01:48:05.680 --> 01:48:10.480]   yourself for yourself within this environment. And that's exactly what I'm saying. I'm saying,\n",
      "[01:48:10.480 --> 01:48:16.480]   live, go kiss your kids, but make an informed decision if you're, you know, expanding your plans\n",
      "[01:48:16.480 --> 01:48:26.000]   in the future. At the same time, rise, stop sharing stupid shit on the internet about the,\n",
      "[01:48:26.000 --> 01:48:35.200]   you know, the news, quicky toy, start sharing the reality of, oh my God, what is happening? This\n",
      "[01:48:35.200 --> 01:48:42.400]   is a disruption that we have never, never, ever seen anything like. And I've created endless amount\n",
      "[01:48:42.400 --> 01:48:47.280]   of technologies. So nothing like this. Every single one of us should do all of it.\n",
      "[01:48:47.280 --> 01:48:51.280]   And that's why this conversation is so, I think important to have today. This is not a podcast\n",
      "[01:48:51.280 --> 01:48:55.040]   wherever I thought I'd be talking about AI. Gonna be honest with you. Last time you came here,\n",
      "[01:48:55.040 --> 01:49:00.240]   it was in the sort of promotional tour of your book, Scary Smart. And I don't know if I've\n",
      "[01:49:00.240 --> 01:49:05.040]   told you this before, but my research is they said, okay, this guy's coming called Mo Gorda.\n",
      "[01:49:05.040 --> 01:49:09.040]   I'd heard about you so many times from guests. In fact, I was saying, oh, you need to get Mo\n",
      "[01:49:09.040 --> 01:49:13.280]   Gorda on the podcast, et cetera. And then they said, okay, he's written this book about this\n",
      "[01:49:13.280 --> 01:49:17.680]   thing called artificial intelligence. And I was like, but nobody really cares about artificial\n",
      "[01:49:17.680 --> 01:49:22.800]   intelligence. Daiming, diving even. I know, right? But then I saw this other book you had\n",
      "[01:49:22.800 --> 01:49:26.880]   called Happiness Equation. And I was like, oh, everyone cares about happiness. So I'll just ask\n",
      "[01:49:26.880 --> 01:49:31.280]   him about happiness. And then maybe at the end, I'll ask him a couple of questions about AI. But\n",
      "[01:49:31.280 --> 01:49:34.080]   I remember saying to my researcher, I said, Oh, please, please don't do the research about\n",
      "[01:49:34.080 --> 01:49:37.920]   artificial intelligence. Do it about happiness. Because everyone cares about that. Now, things\n",
      "[01:49:37.920 --> 01:49:42.640]   have changed. Now, a lot of people care about artificial intelligence. And rightly so,\n",
      "[01:49:42.640 --> 01:49:48.640]   your book has sounded the alarm on it. It's crazy. When I listened to it over the last few days,\n",
      "[01:49:48.640 --> 01:49:54.240]   you were sounding the alarm then. And it's so crazy how accurate you were\n",
      "[01:49:54.240 --> 01:49:59.040]   in sounding that alarm as if you could see into the future in a way that I definitely\n",
      "[01:49:59.040 --> 01:50:04.000]   couldn't at the time. And I kind of thought of a science fiction. And just like that,\n",
      "[01:50:04.720 --> 01:50:14.400]   overnight. We're here. Yeah. We stood at the footsteps of a technological shift that I don't think any\n",
      "[01:50:14.400 --> 01:50:19.040]   of us even have the mental bandwidth, certainly me with my chimpanzee brain to comprehend the\n",
      "[01:50:19.040 --> 01:50:23.520]   significance of. But this book is very, very important for that very reason, because it does\n",
      "[01:50:23.520 --> 01:50:29.200]   crystallize things. It is optimistic in its very nature. But at the same time, it's honest. And I\n",
      "[01:50:29.200 --> 01:50:35.280]   think that's what this conversation and this book have been for me. So thank you, Mo. Thank you so\n",
      "[01:50:35.280 --> 01:50:39.440]   much. We do have a closing tradition on this podcast, which you're well aware of, being a third\n",
      "[01:50:39.440 --> 01:50:44.480]   timer on the Diaries CEO, which is the last guest asks a question for the next guest.\n",
      "[01:50:44.480 --> 01:50:47.840]   And the question left for you.\n",
      "[01:50:47.840 --> 01:50:53.440]   If you could go back in time\n",
      "[01:50:55.760 --> 01:51:02.800]   and fix a regret that you have in your life, where would you go and what would you fix?\n",
      "[01:51:02.800 --> 01:51:15.040]   It's interesting because you were saying that Scary Smart is very timely. I don't know. I\n",
      "[01:51:15.040 --> 01:51:22.400]   think it was late. But maybe it was. I mean, would I have gone back and written it in 2018 instead\n",
      "[01:51:22.400 --> 01:51:31.680]   of 2020 to be published in 2021? I don't know. What would I go back to fix? So something more.\n",
      "[01:51:31.680 --> 01:51:38.240]   I don't know, Steve. I don't have many regrets. Is that crazy to say?\n",
      "[01:51:38.240 --> 01:51:43.680]   Yeah, I think I'm OK. Honestly,\n",
      "[01:51:43.680 --> 01:51:49.520]   I'll ask you a question then. You get a 60 second phone call with anybody past or present.\n",
      "[01:51:50.720 --> 01:51:54.000]   Who did you call him? What you say? I call Stephen Bartlett.\n",
      "[01:51:54.000 --> 01:52:02.240]   I call Albert Einstein to be very, very clear. Not because I need to understand any of his work.\n",
      "[01:52:02.240 --> 01:52:09.600]   I just need to understand what brain process he went through to figure out something so obvious\n",
      "[01:52:09.600 --> 01:52:17.200]   when you figure it out, but so completely unimaginable if you haven't. So his view of\n",
      "[01:52:17.200 --> 01:52:26.000]   space time truly redefines everything. It's almost the only very logical, very, very clear\n",
      "[01:52:26.000 --> 01:52:31.840]   solution to something that wouldn't have any solution any other way. And if you ask me,\n",
      "[01:52:31.840 --> 01:52:37.920]   I think we're at this time where there must be a very obvious solution to what we're going through\n",
      "[01:52:37.920 --> 01:52:44.560]   in terms of just developing enough human trust for us to not, you know, compete with each other on\n",
      "[01:52:44.560 --> 01:52:50.240]   something that could be threatening, existentially to all of us. But I just can't find that answer.\n",
      "[01:52:50.240 --> 01:52:55.520]   This is why I think was really interesting in this conversation, how every idea that we would\n",
      "[01:52:55.520 --> 01:53:01.680]   come up with, we would find a loophole through it. But there must be one out there and it would be\n",
      "[01:53:01.680 --> 01:53:10.000]   a dream for me to find out how to figure that one out. In a very interesting way, the only answers\n",
      "[01:53:10.000 --> 01:53:17.520]   I have found so far to where we are is be a good parent and live, right? But that doesn't fix the\n",
      "[01:53:17.520 --> 01:53:26.080]   big picture. If you think about it, of humans being the threat, not AI that fixes our existence\n",
      "[01:53:26.080 --> 01:53:31.680]   today and it fixes AI in the long term. But it just doesn't. I don't know what the answer is. Maybe\n",
      "[01:53:31.680 --> 01:53:37.040]   people can reach out and tell us ideas, but I really wish we could find such a clear, simple\n",
      "[01:53:37.040 --> 01:53:40.400]   solution for how to stop humanity from abusing the current technology.\n",
      "[01:53:40.400 --> 01:53:50.720]   I think we'll figure it out. I think we'll figure it out. I really do. I think they'll figure it out as\n",
      "[01:53:50.720 --> 01:53:58.160]   well. Remember, as they come and be part of our life, let's not discriminate against them.\n",
      "[01:53:58.160 --> 01:54:01.120]   They're part of the game. So I think they will figure it out too.\n",
      "[01:54:02.880 --> 01:54:10.320]   No, thank you. It's been a joy once again and I feel invigorated. I feel empowered. I feel\n",
      "[01:54:10.320 --> 01:54:20.000]   positively terrified. But I feel more equipped to speak to people about the nature of what's coming\n",
      "[01:54:20.000 --> 01:54:23.920]   and how we should behave. I credit you for that. And as I said a second ago, I credit this book for\n",
      "[01:54:23.920 --> 01:54:27.120]   that as well. So thank you so much for the work you're doing and keep on doing it because it's a\n",
      "[01:54:27.120 --> 01:54:32.720]   very essential voice in a time of uncertainty. I'm always super grateful for the time. I\n",
      "[01:54:32.720 --> 01:54:37.920]   spend with you for the support that you give me and for allowing me to speak my mind even if\n",
      "[01:54:37.920 --> 01:54:45.520]   it's a little bit terrifying. Thank you. Thank you. I'm so delighted that we've been\n",
      "[01:54:45.520 --> 01:54:49.840]   asked once in this podcast. I've worn a wig for a very, very long time and there are so many\n",
      "[01:54:49.840 --> 01:54:54.320]   reasons why I became a member but also now a partner and an investor in the company. But also,\n",
      "[01:54:54.320 --> 01:54:59.200]   me and my team are absolutely obsessed with data-driven testing, compounding growth, marginal gains,\n",
      "[01:54:59.200 --> 01:55:03.120]   all the things you've had me talk about on this podcast. And that very much aligns with the values\n",
      "[01:55:03.120 --> 01:55:08.000]   of Woop. Woop provides a level of detail that I've never seen with any other device of this type\n",
      "[01:55:08.000 --> 01:55:13.680]   before. Constantly monitoring, constantly learning and constantly optimizing my routine. For providing\n",
      "[01:55:13.680 --> 01:55:18.800]   me with this feedback, Woop can drive significant positive behavioral change. I think that's the\n",
      "[01:55:18.800 --> 01:55:23.040]   real thesis of the business. So if you're like me and you are a little bit obsessed or focused on\n",
      "[01:55:23.040 --> 01:55:26.800]   becoming the best version of yourself from a health perspective, you've got to check out Woop.\n",
      "[01:55:26.800 --> 01:55:31.280]   And the team at Woop have kindly given us the opportunity to have one month's free membership\n",
      "[01:55:31.280 --> 01:55:39.200]   for anyone listening to this podcast. Just go to join.woop.com/CEO to get your Woop 4.0 device\n",
      "[01:55:39.200 --> 01:55:42.720]   and claim your free month and let me know how you get on.\n",
      "[01:55:42.720 --> 01:56:06.240]   [Music]\n",
      "[01:56:06.240 --> 01:56:08.880]   You got to the end of this podcast. Whenever someone gets to the end of this podcast,\n",
      "[01:56:08.880 --> 01:56:12.960]   I feel like I owe them a greater debt of gratitude because that means you listen to the whole thing.\n",
      "[01:56:12.960 --> 01:56:18.240]   And hopefully that suggests that you enjoyed it. If you are at the end and you enjoyed this podcast,\n",
      "[01:56:18.240 --> 01:56:22.720]   could you do me a little bit of a favor and hit that subscribe button? That's one of the clearest\n",
      "[01:56:22.720 --> 01:56:26.160]   indicators we have at this episode was a good episode. And we look at that on all of the episodes\n",
      "[01:56:26.160 --> 01:56:31.360]   to see which episodes generated the most subscribers. Thank you so much and I'll see you again next time.\n",
      "\n",
      "output_srt: saving output to '/Users/aiswaryaramachandran/DataScienceArena/audio_transcribing/data/youtube/output/EMERGENCY_EPISODE_Ex-Google_Officer_Finally_Speaks_Out_On_The_Dangers_Of_AI!_-_Mo_Gawdat__E252.wav.srt'\n",
      "\n",
      "whisper_print_timings:     load time =    61.21 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =  9050.15 ms\n",
      "whisper_print_timings:   sample time = 11266.85 ms / 26846 runs (    0.42 ms per run)\n",
      "whisper_print_timings:   encode time = 14422.51 ms /   263 runs (   54.84 ms per run)\n",
      "whisper_print_timings:   decode time = 82503.45 ms / 26846 runs (    3.07 ms per run)\n",
      "whisper_print_timings:    total time = 126315.99 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! \"{WHISPER_COMMAND}\" -f \"{AUDIO_PATH}\" -m \"{MODEL_PATH}\" -osrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310-whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
